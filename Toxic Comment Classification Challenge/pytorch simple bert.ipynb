{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertModel, AdamW, BertPreTrainedModel\n",
    "from transformers import WarmupLinearSchedule as get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are reading the data into the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' 不是內部或外部命令、可執行的程式或批次檔。\n",
      "'unzip' 不是內部或外部命令、可執行的程式或批次檔。\n",
      "'unzip' 不是內部或外部命令、可執行的程式或批次檔。\n",
      "'unzip' 不是內部或外部命令、可執行的程式或批次檔。\n"
     ]
    }
   ],
   "source": [
    "! unzip ../input/jigsaw-toxic-comment-classification-challenge/train.csv.zip;\n",
    "! unzip ../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv.zip;\n",
    "! unzip ../input/jigsaw-toxic-comment-classification-challenge/test.csv.zip;\n",
    "! unzip ../input/jigsaw-toxic-comment-classification-challenge/test_labels.csv.zip;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "bert_model_name = 'bert-base-cased'\n",
    "# path = \"../input/jigsaw-toxic-comment-classification-challenge/\"\n",
    "#device = torch.device('cpu')\n",
    "#if torch.cuda.is_available():\n",
    "device = torch.device('cuda:0')\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "assert tokenizer.pad_token_id == 0, \"Padding value used in masks is set to zero, please change it everywhere\"\n",
    "train_df = pd.read_csv(os.path.join(path, 'train.csv'))\n",
    "# training on a part of data for speed\n",
    "# train_df = train_df.sample(frac=0.33)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>125271</td>\n",
       "      <td>9e0cca8d8ff27220</td>\n",
       "      <td>\"\\nI agree to some of your claims and apprecia...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51442</td>\n",
       "      <td>899c7847276f2f8d</td>\n",
       "      <td>You must be stupid \\n\\nI am referencing an act...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95681</td>\n",
       "      <td>ffe364aa1a4710b2</td>\n",
       "      <td>\"\\n\\nI note that in your request for retention...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111267</td>\n",
       "      <td>533d4a7c82a4ac02</td>\n",
       "      <td>,  a factor of  ≈ 0.999 945 5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57644</td>\n",
       "      <td>9a394d7f24fdb439</td>\n",
       "      <td>Vandalism \\n\\nPlease don't vandalize Wikipedia.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "125271  9e0cca8d8ff27220  \"\\nI agree to some of your claims and apprecia...   \n",
       "51442   899c7847276f2f8d  You must be stupid \\n\\nI am referencing an act...   \n",
       "95681   ffe364aa1a4710b2  \"\\n\\nI note that in your request for retention...   \n",
       "111267  533d4a7c82a4ac02                      ,  a factor of  ≈ 0.999 945 5   \n",
       "57644   9a394d7f24fdb439    Vandalism \\n\\nPlease don't vandalize Wikipedia.   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "125271      0             0        0       0       0              0  \n",
       "51442       1             0        1       0       1              0  \n",
       "95681       0             0        0       0       0              0  \n",
       "111267      0             0        0       0       0              0  \n",
       "57644       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a Dataset and iterators to it for training and validation. It is not truly lazy, as it has dataframe in memory, but they are not converted to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer: BertTokenizer, dataframe: pd.DataFrame, lazy: bool = False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_idx = tokenizer.pad_token_id\n",
    "        self.lazy = lazy\n",
    "        if not self.lazy:\n",
    "            self.X = []\n",
    "            self.Y = []\n",
    "            for i, (row) in tqdm(dataframe.iterrows()):\n",
    "                x, y = self.row_to_tensor(self.tokenizer, row)\n",
    "                self.X.append(x)\n",
    "                self.Y.append(y)\n",
    "        else:\n",
    "            self.df = dataframe        \n",
    "    \n",
    "    @staticmethod\n",
    "    def row_to_tensor(tokenizer: BertTokenizer, row: pd.Series) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        tokens = tokenizer.encode(row[\"comment_text\"], add_special_tokens=True)\n",
    "        if len(tokens) > 120:\n",
    "            tokens = tokens[:119] + [tokens[-1]]\n",
    "        x = torch.LongTensor(tokens)\n",
    "        y = torch.FloatTensor(row[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]])\n",
    "        return x, y\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.lazy:\n",
    "            return len(self.df)\n",
    "        else:\n",
    "            return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "        if not self.lazy:\n",
    "            return self.X[index], self.Y[index]\n",
    "        else:\n",
    "            return self.row_to_tensor(self.tokenizer, self.df.iloc[index])\n",
    "            \n",
    "\n",
    "def collate_fn(batch: List[Tuple[torch.LongTensor, torch.LongTensor]], device: torch.device) \\\n",
    "        -> Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    x, y = list(zip(*batch))\n",
    "    x = pad_sequence(x, batch_first=True, padding_value=0)\n",
    "    y = torch.stack(y)\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "train_dataset = ToxicDataset(tokenizer, train_df, lazy=True)\n",
    "dev_dataset = ToxicDataset(tokenizer, val_df, lazy=True)\n",
    "collate_fn = partial(collate_fn, device=device)\n",
    "BATCH_SIZE = 32\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "dev_sampler = RandomSampler(dev_dataset)\n",
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler, collate_fn=collate_fn)\n",
    "dev_iterator = DataLoader(dev_dataset, batch_size=BATCH_SIZE, sampler=dev_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Bert model for classification of whole sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert: BertModel, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = nn.Linear(bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n",
    "                \n",
    "            labels=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask)\n",
    "        cls_output = outputs[1] # batch, hidden\n",
    "        cls_output = self.classifier(cls_output) # batch, 6\n",
    "        cls_output = torch.sigmoid(cls_output)\n",
    "        criterion = nn.BCELoss()\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = criterion(cls_output, labels)\n",
    "        return loss, cls_output\n",
    "\n",
    "model = BertClassifier(BertModel.from_pretrained(bert_model_name), 6).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and evaluation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        mask = (x != 0).float()\n",
    "        loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(f\"Train loss {total_loss / len(iterator)}\")\n",
    "\n",
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    true = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for x, y in tqdm(iterator):\n",
    "            mask = (x != 0).float()\n",
    "            loss, outputs = model(x, attention_mask=mask, labels=y)\n",
    "            total_loss += loss\n",
    "            true += y.cpu().numpy().tolist()\n",
    "            pred += outputs.cpu().numpy().tolist()\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "    for i, name in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "        print(f\"{name} roc_auc {roc_auc_score(true[:, i], pred[:, i])}\")\n",
    "    print(f\"Evaluate loss {total_loss / len(iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "{'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "EPOCH_NUM = 1\n",
    "# triangular learning rate, linearly grows untill half of first epoch, then linearly decays \n",
    "warmup_steps = 10 ** 3\n",
    "total_steps = len(train_iterator) * EPOCH_NUM - warmup_steps\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "# scheduler = WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps, t_total=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== EPOCH 0 ==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                         | 0/4738 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (554 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (947 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "  0%|                                                                                 | 1/4738 [00:00<37:39,  2.10it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "  0%|                                                                                 | 2/4738 [00:00<36:57,  2.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1055 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "  0%|                                                                                 | 3/4738 [00:01<36:23,  2.17it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1903 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (616 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "  0%|                                                                                 | 4/4738 [00:01<36:02,  2.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "  0%|                                                                                 | 5/4738 [00:02<35:31,  2.22it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "  0%|                                                                                 | 6/4738 [00:02<35:05,  2.25it/s]\n",
      "  0%|                                                                                 | 7/4738 [00:03<34:05,  2.31it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "  0%|▏                                                                                | 8/4738 [00:03<33:57,  2.32it/s]\n",
      "  0%|▏                                                                                | 9/4738 [00:03<34:04,  2.31it/s]\n",
      "  0%|▏                                                                               | 10/4738 [00:04<34:11,  2.30it/s]\n",
      "  0%|▏                                                                               | 11/4738 [00:04<34:23,  2.29it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "for i in range(EPOCH_NUM):\n",
    "    print('=' * 50, f\"EPOCH {i}\", '=' * 50)\n",
    "    train(model, train_iterator, optimizer, scheduler)\n",
    "    evaluate(model, dev_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_df = pd.read_csv(os.path.join(path, 'test.csv'))\n",
    "submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
    "columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "for i in tqdm(range(len(test_df) // BATCH_SIZE + 1)):\n",
    "    batch_df = test_df.iloc[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "    assert (batch_df[\"id\"] == submission[\"id\"][i * BATCH_SIZE: (i + 1) * BATCH_SIZE]).all(), f\"Id mismatch\"\n",
    "    texts = []\n",
    "    for text in batch_df[\"comment_text\"].tolist():\n",
    "        text = tokenizer.encode(text, add_special_tokens=True)\n",
    "        if len(text) > 120:\n",
    "            text = text[:119] + [tokenizer.sep_token_id]\n",
    "        texts.append(torch.LongTensor(text))\n",
    "    x = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id).to(device)\n",
    "    mask = (x != tokenizer.pad_token_id).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        _, outputs = model(x, attention_mask=mask)\n",
    "    outputs = outputs.cpu().numpy()\n",
    "    submission.iloc[i * BATCH_SIZE: (i + 1) * BATCH_SIZE][columns] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
