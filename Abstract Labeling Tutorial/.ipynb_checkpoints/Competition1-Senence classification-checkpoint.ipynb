{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pErN114hmUxF"
   },
   "source": [
    "**人工智慧論文機器閱讀競賽之論文分類(Tagging of Thesis)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TboUaYpis3wb"
   },
   "source": [
    "**Data processing**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yJxx0Tp_s3wc"
   },
   "source": [
    "**刪除多於資訊 (Remove redundant information)**  \n",
    "我們在資料集中保留了許多額外資訊供大家使用，但是在這次的教學中我們並沒有用到全部資訊，因此先將多餘的部分刪除。  \n",
    "In dataset, we reserved lots of information. But the mtehod we use in this tutorial doesn't need them, so let's delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NLc_q3TTs3wc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv('task1_trainset.csv', dtype=str)\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWRHoNZ6s3wi"
   },
   "source": [
    "**資料切割  (Partition)**  \n",
    "在訓練時，我們需要有個方法去檢驗訓練結果的好壞，因此需要將訓練資料切成training/validataion set。   \n",
    "While training, we need some method to exam our model's performance, so we divide our training data into training/validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpEkdlTYs3wj"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "trainset.to_csv('trainset.csv',index=False)\n",
    "validset.to_csv('validset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWsCqgZfs3wl"
   },
   "outputs": [],
   "source": [
    "# Do the same things for test data\n",
    "dataset = pd.read_csv('task1_public_testset.csv', dtype=str)\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset.to_csv('testset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NP4cP79Xs3wn"
   },
   "source": [
    "**統計單字 (Count words)**  \n",
    "在訓練時，不能直接將單字直接餵入model，因為它只看得懂數字，因此我們必須把所有的單字抽取出來，並將它們打上編號，做出一個字典來對它們做轉換。\n",
    "We can't feed \"word\" into model directly, since it can only recognize number. So, we need to know the total number of word, and give every word a unique number.  \n",
    "在這裡，我們需要借助`nltk`這個library來幫忙做文字切割。當然，你也可以選擇自己寫規則來切割(通常上不建議搞死自己)。  \n",
    "In here, we split words by using `nltk library`. You can write your own rules and split it by yourself, but you won't want to do that, trust me.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3192,
     "status": "ok",
     "timestamp": 1572380583236,
     "user": {
      "displayName": "Ming-Hao Wen",
      "photoUrl": "",
      "userId": "00000456228073059477"
     },
     "user_tz": -480
    },
    "id": "VSYXklWwfVUA",
    "outputId": "302344b6-52dd-4e9d-87fe-c0545ae22e37"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# colab doesn't have all package in nltk, we need to download by ourselves.\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxM2Mi8efHcg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iW6Wyv9ZuF7j"
   },
   "outputs": [],
   "source": [
    "def collect_words(data_path):\n",
    "  df = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "  tokens = set()\n",
    "  for i in df.iterrows():\n",
    "    sents  = i[1]['Abstract'].split('$$$')\n",
    "    sents = ' '.join(sents)\n",
    "    tokens |= set(word_tokenize(sents))\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIZf3cwIs3ws"
   },
   "outputs": [],
   "source": [
    "words = set()\n",
    "words |= collect_words('trainset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jq3sPOxls3wu"
   },
   "source": [
    "**建立字典 (Building Dictionary)**  \n",
    "Given every word a unique index.\n",
    "```\n",
    "pad: for padding  \n",
    "unk: for word that not in our dicitonary\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcoIEQdOs3wv"
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "word_dict = {'<pad>':PAD_TOKEN,'<unk>':UNK_TOKEN}\n",
    "for word in words:\n",
    "  word_dict[word]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T2uZ7ziUs3ww"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(dataPath+'dicitonary.pkl','wb') as f:\n",
    "  pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNv7YoAHtlK8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load word dictionary\n",
    "\"\"\"\n",
    "import pickle\n",
    "with open(dataPath+'dicitonary.pkl','rb') as f:\n",
    "  word_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6XU5T-Ls3wy"
   },
   "source": [
    "**資料格式化 (Data formatting)**  \n",
    "有了字典後，接下來我們要把資料整理成一筆一筆，把input的句子轉成數字，把答案轉成onehot的形式。  \n",
    "After building dictionary, that's mapping our sentences into number array, and convert answers to onehot format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cV1auaUs3wz"
   },
   "outputs": [],
   "source": [
    "def get_dataset(data_path, word_dict, n_workers=4):\n",
    "  \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "  Args:\n",
    "      data_path (str): Path to the data.\n",
    "  Return:\n",
    "      output (list of dict): [dict, dict, dict ...]\n",
    "  \"\"\"\n",
    "  dataset = pd.read_csv(data_path, dtype=str)\n",
    "  formatData = []\n",
    "  for (idx,data) in dataset.iterrows():\n",
    "    \"\"\"\n",
    "    processed: {\n",
    "      'Abstract': [[4,5,6],[3,4,2],...]\n",
    "      'Label': [[0,0,0,1,1,0],[1,0,0,0,1,0],...]\n",
    "    }\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "    processed['Abstract'] = [sentence_to_indices(sent, word_dict) for sent in data['Abstract'].split('$$$')]\n",
    "    if 'Task 1' in data:\n",
    "      processed['Label'] = [label_to_onehot(label) for label in data['Task 1'].split(' ')]\n",
    "    formatData.append(processed)\n",
    "  \n",
    "  return formatData\n",
    "  \n",
    "def label_to_onehot(labels):\n",
    "  \"\"\" Convert label to onehot .\n",
    "      Args:\n",
    "          labels (string): sentence's labels.\n",
    "      Return:\n",
    "          outputs (onehot list): sentence's onehot label.\n",
    "  \"\"\"\n",
    "  label_dict = {'BACKGROUND': 0, 'OBJECTIVES':1, 'METHODS':2, 'RESULTS':3, 'CONCLUSIONS':4, 'OTHERS':5}\n",
    "  onehot = [0,0,0,0,0,0]\n",
    "  for l in labels.split('/'):\n",
    "    onehot[label_dict[l]] = 1\n",
    "  return onehot\n",
    "        \n",
    "def sentence_to_indices(sentence, word_dict):\n",
    "  \"\"\" Convert sentence to its word indices.\n",
    "  Args:\n",
    "      sentence (str): One string.\n",
    "  Return:\n",
    "      indices (list of int): List of word indices.\n",
    "  \"\"\"\n",
    "  return [word_dict.get(word,UNK_TOKEN) for word in word_tokenize(sentence)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59199,
     "status": "ok",
     "timestamp": 1572380639326,
     "user": {
      "displayName": "Ming-Hao Wen",
      "photoUrl": "",
      "userId": "00000456228073059477"
     },
     "user_tz": -480
    },
    "id": "ADwpeAfas3w1",
    "outputId": "97b3e826-f021-4ef1-b237-20a65ad9be44",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start processing trainset...\n",
      "[INFO] Start processing validset...\n",
      "[INFO] Start processing testset...\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] Start processing trainset...')\n",
    "train = get_dataset(dataPath+'trainset.csv', word_dict, n_workers=4)\n",
    "print('[INFO] Start processing validset...')\n",
    "valid = get_dataset(dataPath+'validset.csv', word_dict, n_workers=4)\n",
    "print('[INFO] Start processing testset...')\n",
    "test = get_dataset(dataPath+'task1_public_testset.csv', word_dict, n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xf1XoOXTs3w3"
   },
   "source": [
    "**資料封裝 (Data loader)**  \n",
    "為了更方便的進行batch training，我們將會借助[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)。  \n",
    "而要將資料放入dataloader，我們需要繼承[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)，撰寫適合這份dataset的class。  \n",
    "`collate_fn`用於batch data的後處理，在`dataloder`將選出的data放進list後會呼叫collate_fn，而我們會在此把sentence padding到同樣的長度，才能夠放入torch tensor (tensor必須為矩陣)。  \n",
    "To easily training in batch, we'll use `dataloader`, which is a function built in Pytorch [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)  \n",
    "To use datalaoder, we need to packing our data into class `dataset` [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)  \n",
    "`collate_fn` is used for data processing, and we will padding sentence in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3uauKS1s3w4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Don't forget the data type that tidy up from get_dataset()!! \n",
    "Format: [dict, dict, ...]\n",
    "dict:{\n",
    "  'Abstract': [[4,5,6],[3,4,2],...]\n",
    "  'Label': [[0,0,0,1,1,0],[1,0,0,0,1,0],...] <-- testing data don't have this\n",
    "}\n",
    "\"\"\"\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "class AbstractDataset(Dataset):\n",
    "  def __init__(self, data, pad_idx, max_len = 500):\n",
    "    self.data = data\n",
    "    self.pad_idx = pad_idx\n",
    "    self.max_len = max_len\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    return self.data[index]\n",
    "      \n",
    "  def collate_fn(self, datas):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    Tensor(batch,sentence,words) : input data\n",
    "    Tensor(batch,sentence,words) : corresponding answer\n",
    "    list(sentence quantity in each abstract): use in prediction, to remove the redundant sentences (the sentences we padded)\n",
    "    \n",
    "    \"\"\"\n",
    "    # get max length in this batch\n",
    "    max_sent = max([len(data['Abstract']) for data in datas])\n",
    "    max_len = max([min(len(sentence), self.max_len) for data in datas for sentence in data['Abstract']])\n",
    "    batch_abstract = []\n",
    "    batch_label = []\n",
    "    sent_len = []\n",
    "    for data in datas:\n",
    "      # padding abstract to make them in same length\n",
    "      pad_abstract = []\n",
    "      for sentence in data['Abstract']:\n",
    "        if len(sentence) > max_len:\n",
    "          pad_abstract.append(sentence[:max_len])\n",
    "        else:\n",
    "          pad_abstract.append(sentence+[self.pad_idx]*(max_len-len(sentence)))\n",
    "      sent_len.append(len(pad_abstract))\n",
    "      pad_abstract.extend([[self.pad_idx]*max_len]*(max_sent-len(pad_abstract)))\n",
    "      batch_abstract.append(pad_abstract)\n",
    "\n",
    "      # gather labels\n",
    "      if 'Label' in data:\n",
    "          pad_label = data['Label']\n",
    "          pad_label.extend([[0]*6]*(max_sent-len(pad_label)))\n",
    "          batch_label.append(pad_label)\n",
    "\n",
    "    return torch.LongTensor(batch_abstract), torch.FloatTensor(batch_label), sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyugmkAjs3w6"
   },
   "outputs": [],
   "source": [
    "trainData = AbstractDataset(train, PAD_TOKEN, max_len = 64)\n",
    "validData = AbstractDataset(valid, PAD_TOKEN, max_len = 64)\n",
    "testData = AbstractDataset(test, PAD_TOKEN, max_len = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwtiAwqes3w8"
   },
   "source": [
    "**Model**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rWa0P89_s3w9"
   },
   "source": [
    "資料處理完成後，接下來就是最重要的核心部分：`Model`。  \n",
    "此次範例中我們以簡單的一層RNN + 兩層Linear layer作為示範。  \n",
    "而為了解決每次的句子長度不一的問題(`linear layer必須是fixed input size`)，因此我們取所有hidden_state裡, 每一個feature的最大值，讓這一個vector代表這句話。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcfgVFE9s3w_"
   },
   "source": [
    "In this tutorial, we're going to implement a simple model, which contain one RNN layer and two fully connected layers (Linear layer). Of course you can make it \"deep\".  \n",
    "To solve variant sentence length problem (`input size in linear layer must be fixed`), we can average all hidden_states or doing max pooling, and become one vector. (Perfect!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGuZR0ifs3xA"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class simpleNet(nn.Module):\n",
    "  def __init__(self, vocabulary_size):\n",
    "    super(simpleNet, self).__init__()\n",
    "    self.embedding_size = 50\n",
    "    self.hidden_dim = 512\n",
    "    self.embedding = nn.Embedding(vocabulary_size, self.embedding_size)\n",
    "    self.sent_rnn = nn.GRU(self.embedding_size,\n",
    "                            self.hidden_dim,\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "    self.l1 = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n",
    "    self.l2 = nn.Linear(self.hidden_dim, 6)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x: (batch,sent,word)\n",
    "    x = self.embedding(x)\n",
    "    # x: (batch,sent,word,feature)\n",
    "    b,s,w,e = x.shape\n",
    "    x = x.view(b,s*w,e)\n",
    "    # x: (batch,sent*word,feature)\n",
    "    x, __ = self.sent_rnn(x)\n",
    "    # x: (batch,sent*word,hidden_state*2)\n",
    "    x = x.view(b,s,w,-1)\n",
    "    # x: (batch,sent,word,hidden_state*2)\n",
    "    x = torch.max(x,dim=2)[0]\n",
    "    # x: (batch,sent,hidden_state*2)\n",
    "    x = torch.relu(self.l1(x))\n",
    "    x = torch.sigmoid(self.l2(x))\n",
    "    # x: (batch,sent,6)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpfH3pVxs3xC"
   },
   "source": [
    "**Training**  \n",
    "指定使用的運算裝置  \n",
    "Designate running device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UPABKsns3xD"
   },
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DemqkWvzs3xF"
   },
   "source": [
    "定義計分公式, 讓我們在training能快速了解model的效能  \n",
    "Define score function, let us easily observe model performance while training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0NqkVcas3xG"
   },
   "outputs": [],
   "source": [
    "class F1():\n",
    "  def __init__(self):\n",
    "    self.threshold = 0.5\n",
    "    self.n_precision = 0\n",
    "    self.n_recall = 0\n",
    "    self.n_corrects = 0\n",
    "    self.name = 'F1'\n",
    "\n",
    "  def reset(self):\n",
    "    self.n_precision = 0\n",
    "    self.n_recall = 0\n",
    "    self.n_corrects = 0\n",
    "\n",
    "  def update(self, predicts, groundTruth):\n",
    "    predicts = (predicts > self.threshold).float()\n",
    "    self.n_precision += torch.sum(predicts).data.item()\n",
    "    self.n_recall += torch.sum(groundTruth).data.item()\n",
    "    self.n_corrects += torch.sum(groundTruth * predicts).data.item()\n",
    "\n",
    "  def get_score(self):\n",
    "    recall = self.n_corrects / self.n_recall\n",
    "    precision = self.n_corrects / (self.n_precision + 1e-20) #prevent divided by zero\n",
    "    return 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "\n",
    "  def print_score(self):\n",
    "    score = self.get_score()\n",
    "    return '{:.5f}'.format(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgiLl8E0s3xJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "def _run_epoch(epoch, training):\n",
    "  model.train(training)\n",
    "  if training:\n",
    "    description = 'Train'\n",
    "    dataset = trainData\n",
    "    shuffle = True\n",
    "  else:\n",
    "    description = 'Valid'\n",
    "    dataset = validData\n",
    "    shuffle = False\n",
    "  dataloader = DataLoader(dataset=dataset,\n",
    "                          batch_size=64,\n",
    "                          shuffle=shuffle,\n",
    "                          collate_fn=dataset.collate_fn,\n",
    "                          num_workers=4)\n",
    "\n",
    "  trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n",
    "  loss = 0\n",
    "  f1_score = F1()\n",
    "  for i, (x, y, sent_len) in trange:\n",
    "    opt.zero_grad()\n",
    "\n",
    "    abstract = x.to(device)\n",
    "    labels = y.to(device)\n",
    "    o_labels = model(abstract)\n",
    "    batch_loss = criteria(o_labels, labels)\n",
    "\n",
    "    if training:\n",
    "      batch_loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "    loss += batch_loss.item()\n",
    "    f1_score.update(o_labels.cpu(), y)\n",
    "\n",
    "    trange.set_postfix(\n",
    "      loss=loss / (i + 1), f1=f1_score.print_score())\n",
    "  if training:\n",
    "      history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "  else:\n",
    "      history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "\n",
    "\n",
    "def save(epoch):\n",
    "  if not os.path.exists(dataPath+'model'):\n",
    "    os.makedirs(dataPath+'model')\n",
    "        \n",
    "  torch.save(model.state_dict(), dataPath+'model/model.pkl.'+str(epoch))\n",
    "  with open(dataPath+'model/history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dTzL36Es3xL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "model = simpleNet(len(word_dict))\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "criteria = torch.nn.BCELoss()\n",
    "model.to(device)\n",
    "max_epoch = 7\n",
    "history = {'train':[],'valid':[]}\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "  print('Epoch: {}'.format(epoch))\n",
    "  _run_epoch(epoch, True)\n",
    "  _run_epoch(epoch, False)\n",
    "  save(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ndxm6R7s3xS"
   },
   "source": [
    "**Plot Learning Curve**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aBIC6eXQs3xS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with open(dataPath+'model/history.json', 'r') as f:\n",
    "    history = json.loads(f.read())\n",
    "    \n",
    "train_loss = [l['loss'] for l in history['train']]\n",
    "valid_loss = [l['loss'] for l in history['valid']]\n",
    "train_f1 = [l['f1'] for l in history['train']]\n",
    "valid_f1 = [l['f1'] for l in history['valid']]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Loss')\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(valid_loss, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('F1 Score')\n",
    "plt.plot(train_f1, label='train')\n",
    "plt.plot(valid_f1, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Best F1 score ', max([[l['f1'], idx] for idx, l in enumerate(history['valid'])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACtyMea_NXnB"
   },
   "source": [
    "**Choose your best model according to learning graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Oo3NsRoB-k_"
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(dataPath+'model/model.pkl.6'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J1Vn5fdos3xM"
   },
   "source": [
    "**Predict**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DjYZqo3ws3xN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train(False)\n",
    "dataloader = DataLoader(dataset=testData,\n",
    "                            batch_size=64,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=testData.collate_fn,\n",
    "                            num_workers=4)\n",
    "trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n",
    "prediction = []\n",
    "for i, (x, y, sent_len) in trange:\n",
    "  o_labels = model(x.to(device))\n",
    "  o_labels = o_labels>0.5\n",
    "  for idx, o_label in enumerate(o_labels):\n",
    "    prediction.append(o_label[:sent_len[idx]].to('cpu'))\n",
    "prediction = torch.cat(prediction).detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPIDfsDps3xP"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Public = True if you're predicting public test data.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n",
    "    sample = pd.read_csv(sampleFile)\n",
    "    submit = {}\n",
    "    submit['order_id'] = list(sample.order_id.values)\n",
    "    redundant = len(sample) - prediction.shape[0]\n",
    "    if public:\n",
    "      submit['BACKGROUND'] = list(prediction[:,0]) + [0]*redundant\n",
    "      submit['OBJECTIVES'] = list(prediction[:,1]) + [0]*redundant\n",
    "      submit['METHODS'] = list(prediction[:,2]) + [0]*redundant\n",
    "      submit['RESULTS'] = list(prediction[:,3]) + [0]*redundant\n",
    "      submit['CONCLUSIONS'] = list(prediction[:,4]) + [0]*redundant\n",
    "      submit['OTHERS'] = list(prediction[:,5]) + [0]*redundant\n",
    "    else:\n",
    "      submit['BACKGROUND'] = [0]*redundant + list(prediction[:,0])\n",
    "      submit['OBJECTIVES'] = [0]*redundant + list(prediction[:,1])\n",
    "      submit['METHODS'] = [0]*redundant + list(prediction[:,2])\n",
    "      submit['RESULTS'] = [0]*redundant + list(prediction[:,3])\n",
    "      submit['CONCLUSIONS'] = [0]*redundant + list(prediction[:,4])\n",
    "      submit['OTHERS'] = [0]*redundant + list(prediction[:,5])\n",
    "    df = pd.DataFrame.from_dict(submit) \n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6MFSM6Wls3xQ"
   },
   "outputs": [],
   "source": [
    "SubmitGenerator(prediction,dataPath+'task1_sample_submission.csv',True, dataPath+'task1_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Competition1-Senence classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
