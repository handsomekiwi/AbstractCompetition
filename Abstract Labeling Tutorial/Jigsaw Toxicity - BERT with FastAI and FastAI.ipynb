{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I want to use two state of the art Natural Language Processing (NLP) techniques which have sort of revolutionalized the area of NLP in Deep Learning.\n",
    "\n",
    "These techniques are as follows:\n",
    "\n",
    "1. BERT (Deep Bidirectional Transformers for Language Understanding)\n",
    "2. Fastai ULMFiT (Universal Language Model Fine-tuning for Text Classification)\n",
    "\n",
    "Both these techniques are very advanced and very recent NLP techniques (BERT was introduced by Google in 2018). Both of them incorporate the methods of Transfer Learning which is quite cool and are pre-trained on large corpuses of Wikipedia articles. I wanted to compare the overall performance of these two techniques.\n",
    "\n",
    "I really like using Fastai for my deep learning projects and can't thank enough for this amazing community and our mentors - Jeremy & Rachael for creating few wonderful courses on the matters pertaining to Deep Learning. Therefore one of my aims to work on this project was to **integrate BERT with Fastai**. This means power of BERT combined with the simplicity of Fastai. It was not an easy task especially implementing Discriminative Learning Rate technique of Fastai in BERT modelling. \n",
    "\n",
    "In my project, below article helped me in understanding few of these integration techniques and I would like to extend my gratidue to the writer of this article:\n",
    "\n",
    "[https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/](http://)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we will use Jigsaw's Toxic Comments dataset which has categorized each text item into 6 classes -\n",
    "\n",
    "1. Toxic\n",
    "2. Severe Toxic\n",
    "3. Obscene\n",
    "4. Threat\n",
    "5. Insult\n",
    "6. Identity Hate\n",
    "\n",
    "This is a **multi-label text classification challenge**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will import Fastai libraries and few other important libraries for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pretrainedmodels\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "#!pip install fastai==1.0.57\n",
    "import fastai\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.text import *\n",
    "\n",
    "from torchvision.models import *\n",
    "import pretrainedmodels\n",
    "\n",
    "from utils import *\n",
    "import sys\n",
    "\n",
    "from fastai.callbacks.tracker import EarlyStoppingCallback\n",
    "from fastai.callbacks.tracker import SaveModelCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import Huggingface's \"pytorch-pretrained-bert\" model (this is now renamed as pytorch-transformers)\n",
    "\n",
    "[https://github.com/huggingface/pytorch-transformers](http://)\n",
    "\n",
    "This is a brilliant repository of few of amazing NLP techniques and already pre-trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT has several flavours when it comes to Tokenization. For our modelling purposes, we will use the most common and standard method named as \"bert-case-uncased\".\n",
    "\n",
    "We will name this as bert_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "bert_tok = BertTokenizer.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "torch.cuda.set_device(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the article in first section, we will change the tokenizer of Fastai to incorporate BertTokenizer. One important thing to note here is to change the start and end of each token with [CLS] and [SEP] which is a requirement of BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastAiBertTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length\"\"\"\n",
    "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move further, lets have a look at the Data on which we have to work.\n",
    "\n",
    "We will split the train data into two parts: Train, Validation. However, for the purpose of this project, we will not be using Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"\")\n",
    "\n",
    "train, test = [pd.read_csv(DATA_ROOT / fname) for fname in [\"train.csv\", \"test.csv\"]]\n",
    "train, val = train_test_split(train, shuffle=True, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140030</th>\n",
       "      <td>ed56f082116dcbd0</td>\n",
       "      <td>Grandma Terri Should Burn in Trash \\nGrandma T...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159124</th>\n",
       "      <td>f8e3cd98b63bf401</td>\n",
       "      <td>, 9 May 2009 (UTC)\\nIt would be easiest if you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60006</th>\n",
       "      <td>a09e1bcf10631f9a</td>\n",
       "      <td>\"\\n\\nThe Objectivity of this Discussion is dou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65432</th>\n",
       "      <td>af0ee0066c607eb8</td>\n",
       "      <td>Shelly Shock\\nShelly Shock is. . .( )</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154979</th>\n",
       "      <td>b734772b1a807e09</td>\n",
       "      <td>I do not care. Refer to Ong Teng Cheong talk p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "140030  ed56f082116dcbd0  Grandma Terri Should Burn in Trash \\nGrandma T...   \n",
       "159124  f8e3cd98b63bf401  , 9 May 2009 (UTC)\\nIt would be easiest if you...   \n",
       "60006   a09e1bcf10631f9a  \"\\n\\nThe Objectivity of this Discussion is dou...   \n",
       "65432   af0ee0066c607eb8              Shelly Shock\\nShelly Shock is. . .( )   \n",
       "154979  b734772b1a807e09  I do not care. Refer to Ong Teng Cheong talk p...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "140030      1             0        0       0       0              0  \n",
       "159124      0             0        0       0       0              0  \n",
       "60006       0             0        0       0       0              0  \n",
       "65432       0             0        0       0       0              0  \n",
       "154979      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119105</th>\n",
       "      <td>7ca72b5b9c688e9e</td>\n",
       "      <td>Geez, are you forgetful!  We've already discus...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131631</th>\n",
       "      <td>c03f72fd8f8bf54f</td>\n",
       "      <td>Carioca RFA \\n\\nThanks for your support on my ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125326</th>\n",
       "      <td>9e5b8e8fc1ff2e84</td>\n",
       "      <td>\"\\n\\n Birthday \\n\\nNo worries, It's what I do ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111256</th>\n",
       "      <td>5332799e706665a6</td>\n",
       "      <td>Pseudoscience category? \\n\\nI'm assuming that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83590</th>\n",
       "      <td>dfa7d8f0b4366680</td>\n",
       "      <td>(and if such phrase exists, it would be provid...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "119105  7ca72b5b9c688e9e  Geez, are you forgetful!  We've already discus...   \n",
       "131631  c03f72fd8f8bf54f  Carioca RFA \\n\\nThanks for your support on my ...   \n",
       "125326  9e5b8e8fc1ff2e84  \"\\n\\n Birthday \\n\\nNo worries, It's what I do ...   \n",
       "111256  5332799e706665a6  Pseudoscience category? \\n\\nI'm assuming that ...   \n",
       "83590   dfa7d8f0b4366680  (and if such phrase exists, it would be provid...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "119105      0             0        0       0       0              0  \n",
       "131631      0             0        0       0       0              0  \n",
       "125326      0             0        0       0       0              0  \n",
       "111256      0             0        0       0       0              0  \n",
       "83590       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In following code snippets, we need to wrap BERT vocab and BERT tokenizer with Fastai modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=256), pre_rules=[], post_rules=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create our Databunch. Important thing to note here is to use BERT Tokenizer, BERT Vocab. And to and put include_bos and include_eos as False as Fastai puts some default values for these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "databunch_1 = TextDataBunch.from_df(\".\", train, val, \n",
    "                  tokenizer=fastai_tokenizer,\n",
    "                  vocab=fastai_bert_vocab,\n",
    "                  include_bos=False,\n",
    "                  include_eos=False,\n",
    "                  text_cols=\"comment_text\",\n",
    "                  label_cols=label_cols,\n",
    "                  bs=32,\n",
    "                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can pass our own list of Preprocessors to the databunch (this is effectively what is happening behind the scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizeProcessor(TokenizeProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
    "\n",
    "class BertNumericalizeProcessor(NumericalizeProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, vocab=Vocab(list(bert_tok.vocab.keys())), **kwargs)\n",
    "\n",
    "def get_bert_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
    "    \"\"\"\n",
    "    Constructing preprocessors for BERT\n",
    "    We remove sos/eos tokens since we add that ourselves in the tokenizer.\n",
    "    We also use a custom vocabulary to match the numericalization with the original BERT model.\n",
    "    \"\"\"\n",
    "    return [BertTokenizeProcessor(tokenizer=tokenizer),\n",
    "            NumericalizeProcessor(vocab=vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataBunch(TextDataBunch):\n",
    "    @classmethod\n",
    "    def from_df(cls, path:PathOrStr, train_df:DataFrame, valid_df:DataFrame, test_df:Optional[DataFrame]=None,\n",
    "                tokenizer:Tokenizer=None, vocab:Vocab=None, classes:Collection[str]=None, text_cols:IntsOrStrs=1,\n",
    "                label_cols:IntsOrStrs=0, label_delim:str=None, **kwargs) -> DataBunch:\n",
    "        \"Create a `TextDataBunch` from DataFrames.\"\n",
    "        p_kwargs, kwargs = split_kwargs_by_func(kwargs, get_bert_processor)\n",
    "        # use our custom processors while taking tokenizer and vocab as kwargs\n",
    "        processor = get_bert_processor(tokenizer=tokenizer, vocab=vocab, **p_kwargs)\n",
    "        if classes is None and is_listy(label_cols) and len(label_cols) > 1: classes = label_cols\n",
    "        src = ItemLists(path, TextList.from_df(train_df, path, cols=text_cols, processor=processor),\n",
    "                        TextList.from_df(valid_df, path, cols=text_cols, processor=processor))\n",
    "        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_df(cols=label_cols, classes=classes)\n",
    "        if test_df is not None: src.add_test(TextList.from_df(test_df, path, cols=text_cols))\n",
    "        return src.databunch(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this will produce a virtually identical databunch to the code above\n",
    "databunch_2 = BertDataBunch.from_df(\".\", train_df=train, valid_df=val,\n",
    "                  tokenizer=fastai_tokenizer,\n",
    "                  vocab=fastai_bert_vocab,\n",
    "                  text_cols=\"comment_text\",\n",
    "                  label_cols=label_cols,\n",
    "                  bs=32,\n",
    "                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=Path('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[CLS] \" speedy del ##eti ##on of phil ##vid ##ler ##66 ##6 a tag has been placed on phil ##vid ##ler ##66 ##6 requesting that it be speed ##ily deleted from wikipedia . this has been done under section a ##7 of the criteria for speedy del ##eti ##on , because the article appears to be about a person or group of people , but it does not indicate how</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] \" responses i am not in a position where i will be able to correspond extensively on this matter , but i thought i owed it to everyone to try and put together a few general responses . yes , i should not have used the \" \" are you now or have you ever been ? \" \" sort of construction , and i ' ve modified it</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] \" regarding the suit ##ability of the rb ##s - 15 for different targets = = = as noted when someone did undo my edit that removed the statement about the rb ##s - 15 being \" \" just too large ( and too expensive ) for most targets \" \" , this statement in the text was supposed to be referring to land targets . however the meaning</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] queen mother of the west hello , and welcome to wikipedia ! we welcome and appreciate your contributions , such as queen mother of the west , but we regret ##fully cannot accept copyright ##ed text or images borrowed from either web sites or printed material . this article appears to be a direct copy from http : / / www . eng . tao ##ism . org .</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] \" _ _ no ##ind ##ex _ _ wikipedia _ talk : hat ##note thanks for your suggestion . i ' ve made my case on the talk page . ( wikipedia _ talk : hat ##note # trivial _ hat ##note _ links ) please join if you feel inclined . regards , - ( t | c ) bk ##on ##rad i can understand some changes to</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "databunch_2.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>[CLS] \" reviewer hello . your account has been granted the \" \" reviewer \" \" user ##right , allowing you to review other users ' edit ##s on certain flag ##ged pages . pending changes , also known as flag ##ged revisions , underwent a two - month trial which ended on 15 august 2010 . its continued use is still being discussed by the community , you are</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] \" similar statement was made about methane . i revised using \" \" widely considered \" \" and \" \" purported \" \" to be consistent with the idea of making the statements less absolute . i recognize weasel word ##ing dilemma ##s , but seriously , this accurately portrays the qu ##ali ##tative information without misleading . not necessary . the \" \" debate \" \" over global</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] \" corporal ##s _ killings = = i assume , from your edit ##s , that you have edited on w ##p before under another name . however , should this not be the case , i would respectful ##ly point out that the led ##e in a w ##p article should normally en ##cap ##sul ##ate the article . in the above article the subsequent passages make clear</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] \" an ##i discussion i am currently blocked and unable to properly defend myself at a discussion at w ##p : an ##i . i request that a neutral ad ##min copy this content to that area . writes : \" \" yet again , i ask led ##rus ##h , the 152 . x . x . x ip , or anyone else , to point out any</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>[CLS] \" september 2013 ( utc ) i love how i specifically tried in a polite manner to engage in meaningful dialogue here , and asked you to respond in a way to help me understand why you refuse to accept the submission . your 2 - line response was the equivalent of \" \" nana - nana boo - boo , \" \" as far as i am concerned</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "databunch_1.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Databunch_1 and Databunch_2 can be used for modelling purposes. In this project, we will be using Databunch_1 which is easier to create and use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification, BertForNextSentencePrediction, BertForMaskedLM\n",
    "bert_model_class = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function to be used is Binary Cross Entropy with Logistic Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering this is a multi-label classification problem, we cant use simple accuracy as metrics here. Instead, we will use accuracy_thresh with threshold of 25% as our metric here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_02 = partial(accuracy_thresh, thresh=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bert_model_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets create learner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callbacks import *\n",
    "\n",
    "learner = Learner(\n",
    "    databunch_1, model,\n",
    "    loss_func=loss_func, model_dir='model', metrics=acc_02,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code will help us in splitting the model into desirable parts which will be helpful for us in Discriminative Learning i.e. setting up different learning rates and weight decays for different parts of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_clas_split(self) -> List[nn.Module]:\n",
    "    \n",
    "    bert = model.bert\n",
    "    embedder = bert.embeddings\n",
    "    pooler = bert.pooler\n",
    "    encoder = bert.encoder\n",
    "    classifier = [model.dropout, model.classifier]\n",
    "    n = len(encoder.layer)//3\n",
    "    print(n)\n",
    "    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "x = bert_clas_split(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the model now in 6 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash . i hate grandma terri . f % % k her to hell ! 71 . 74 . 76 . 40 [SEP],[CLS] , 9 may 2009 ( utc ) it would be easiest if you were to admit to being a member of the involved portuguese lodge , and then there would be no requirement to acknowledge whether you had a previous account ( carlos bot ##el ##ho did not have a good record ) or not and i would then remove the sock ##pu ##ppet template as irrelevant . w ##p : co ##i permits people to edit those articles , such as ms ##ja ##pan does , but just means you have to be more careful in ensuring that references back your edit ##s and that np ##ov is upheld . 20 : 29 [SEP],[CLS] \" the object ##ivity of this discussion is doubtful ( non - existent ) ( 1 ) as indicated earlier , the section on marxist leaders ’ views is misleading : ( a ) it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky , creating the misleading impression that other prominent marxist ##s ( marx , eng ##els , lenin ) did not advocate and / or practiced terrorism ; ( b ) it lays un ##war ##rant ##ed and excessive emphasis on the theoretical “ rejection of individual terrorism ” , creating the misleading impression that this is the main ( only ) marxist position on terrorism . ( 2 ) the discussion is not being properly monitored : ( a ) no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity ; ( b ) important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion ; ( c ) though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by / on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion . ( 3 ) among the tactics deployed by [SEP],[CLS] shelly shock shelly shock is . . . ( ) [SEP],[CLS] i do not care . refer to on ##g ten ##g che ##ong talk page . is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions . she is making use of the dead to push her agenda again . right before elections too . how timely . 202 . 156 . 13 . 232 [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z , are you forget ##ful ! we ' ve already discussed why marx was not an anarchist , i . e . he wanted to use a state to mold his ' socialist man . ' er ##go , he is a stat ##ist - the opposite of an anarchist . i know a guy who says that , when he gets old and his teeth fall out , he ' ll quit eating meat . would you call him a vegetarian ? [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip . the final outcome was ( 31 / 4 / 1 ) , so i am now an administrator . if you have any comments or concerns on my actions as an administrator , please let me know . thank you ! [SEP],[CLS] \" birthday no worries , it ' s what i do ; ) enjoy ur day | talk | e \" [SEP],[CLS] pseudo ##sc ##ience category ? i ' m assuming that this article is in the pseudo ##sc ##ience category because of its association with creation ##ism . however , there are modern , scientific ##ally - accepted variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism — and they ' re even mentioned in the article ! i think the connection to pseudo ##sc ##ience needs to be clarified , or the article made more general and less creation ##ism - specific and the category tag removed entirely . [SEP],[CLS] ( and if such phrase exists , it would be provided by search engine even if mentioned page is not available as a whole ) [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7ff5da21fa70>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Dropout(p=0.1, inplace=False)\n",
       "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.split([x[0], x[1], x[2], x[3], x[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.048037</td>\n",
       "      <td>0.041052</td>\n",
       "      <td>0.978083</td>\n",
       "      <td>22:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.034995</td>\n",
       "      <td>0.036652</td>\n",
       "      <td>0.981048</td>\n",
       "      <td>20:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash . i hate grandma terri . f % % k her to hell ! 71 . 74 . 76 . 40 [SEP],[CLS] , 9 may 2009 ( utc ) it would be easiest if you were to admit to being a member of the involved portuguese lodge , and then there would be no requirement to acknowledge whether you had a previous account ( carlos bot ##el ##ho did not have a good record ) or not and i would then remove the sock ##pu ##ppet template as irrelevant . w ##p : co ##i permits people to edit those articles , such as ms ##ja ##pan does , but just means you have to be more careful in ensuring that references back your edit ##s and that np ##ov is upheld . 20 : 29 [SEP],[CLS] \" the object ##ivity of this discussion is doubtful ( non - existent ) ( 1 ) as indicated earlier , the section on marxist leaders ’ views is misleading : ( a ) it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky , creating the misleading impression that other prominent marxist ##s ( marx , eng ##els , lenin ) did not advocate and / or practiced terrorism ; ( b ) it lays un ##war ##rant ##ed and excessive emphasis on the theoretical “ rejection of individual terrorism ” , creating the misleading impression that this is the main ( only ) marxist position on terrorism . ( 2 ) the discussion is not being properly monitored : ( a ) no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity ; ( b ) important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion ; ( c ) though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by / on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion . ( 3 ) among the tactics deployed by [SEP],[CLS] shelly shock shelly shock is . . . ( ) [SEP],[CLS] i do not care . refer to on ##g ten ##g che ##ong talk page . is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions . she is making use of the dead to push her agenda again . right before elections too . how timely . 202 . 156 . 13 . 232 [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z , are you forget ##ful ! we ' ve already discussed why marx was not an anarchist , i . e . he wanted to use a state to mold his ' socialist man . ' er ##go , he is a stat ##ist - the opposite of an anarchist . i know a guy who says that , when he gets old and his teeth fall out , he ' ll quit eating meat . would you call him a vegetarian ? [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip . the final outcome was ( 31 / 4 / 1 ) , so i am now an administrator . if you have any comments or concerns on my actions as an administrator , please let me know . thank you ! [SEP],[CLS] \" birthday no worries , it ' s what i do ; ) enjoy ur day | talk | e \" [SEP],[CLS] pseudo ##sc ##ience category ? i ' m assuming that this article is in the pseudo ##sc ##ience category because of its association with creation ##ism . however , there are modern , scientific ##ally - accepted variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism — and they ' re even mentioned in the article ! i think the connection to pseudo ##sc ##ience needs to be clarified , or the article made more general and less creation ##ism - specific and the category tag removed entirely . [SEP],[CLS] ( and if such phrase exists , it would be provided by search engine even if mentioned page is not available as a whole ) [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7ff5da21fa70>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Dropout(p=0.1, inplace=False)\n",
       "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.save('head')\n",
    "learner.load('head')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will unfreeze last two last layers and train the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.033646</td>\n",
       "      <td>0.038024</td>\n",
       "      <td>0.981905</td>\n",
       "      <td>11:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.033887</td>\n",
       "      <td>0.036767</td>\n",
       "      <td>0.981425</td>\n",
       "      <td>12:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.freeze_to(-2)\n",
    "learner.fit_one_cycle(2, max_lr=slice(1e-5, 5e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Learner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "[CLS] grandma terri should burn in trash grandma terri is trash . i hate grandma terri . f % % k her to hell ! 71 . 74 . 76 . 40 [SEP],[CLS] , 9 may 2009 ( utc ) it would be easiest if you were to admit to being a member of the involved portuguese lodge , and then there would be no requirement to acknowledge whether you had a previous account ( carlos bot ##el ##ho did not have a good record ) or not and i would then remove the sock ##pu ##ppet template as irrelevant . w ##p : co ##i permits people to edit those articles , such as ms ##ja ##pan does , but just means you have to be more careful in ensuring that references back your edit ##s and that np ##ov is upheld . 20 : 29 [SEP],[CLS] \" the object ##ivity of this discussion is doubtful ( non - existent ) ( 1 ) as indicated earlier , the section on marxist leaders ’ views is misleading : ( a ) it lays un ##war ##rant ##ed and excessive emphasis on tr ##ots ##ky , creating the misleading impression that other prominent marxist ##s ( marx , eng ##els , lenin ) did not advocate and / or practiced terrorism ; ( b ) it lays un ##war ##rant ##ed and excessive emphasis on the theoretical “ rejection of individual terrorism ” , creating the misleading impression that this is the main ( only ) marxist position on terrorism . ( 2 ) the discussion is not being properly monitored : ( a ) no disc ##ern ##ible attempt is being made to establish and maintain an acceptable degree of object ##ivity ; ( b ) important and relevant scholarly works such as the international encyclopedia of terrorism are being ignored or illicit ##ly excluded from the discussion ; ( c ) though the only logical way to remedy the b ##lat ##ant im ##balance in the above section is to include quotes by / on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with imp ##uni ##ty by the ap ##ologists for marxist terrorism who have done their best to sabotage and wreck both the article and the discussion . ( 3 ) among the tactics deployed by [SEP],[CLS] shelly shock shelly shock is . . . ( ) [SEP],[CLS] i do not care . refer to on ##g ten ##g che ##ong talk page . is la go ##ut ##te de pl ##ui ##e writing a biography or writing the history of trade unions . she is making use of the dead to push her agenda again . right before elections too . how timely . 202 . 156 . 13 . 232 [SEP]\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "[CLS] gee ##z , are you forget ##ful ! we ' ve already discussed why marx was not an anarchist , i . e . he wanted to use a state to mold his ' socialist man . ' er ##go , he is a stat ##ist - the opposite of an anarchist . i know a guy who says that , when he gets old and his teeth fall out , he ' ll quit eating meat . would you call him a vegetarian ? [SEP],[CLS] car ##io ##ca rf ##a thanks for your support on my request for ad ##mins ##hip . the final outcome was ( 31 / 4 / 1 ) , so i am now an administrator . if you have any comments or concerns on my actions as an administrator , please let me know . thank you ! [SEP],[CLS] \" birthday no worries , it ' s what i do ; ) enjoy ur day | talk | e \" [SEP],[CLS] pseudo ##sc ##ience category ? i ' m assuming that this article is in the pseudo ##sc ##ience category because of its association with creation ##ism . however , there are modern , scientific ##ally - accepted variants of cat ##ast ##rop ##hism that have nothing to do with creation ##ism — and they ' re even mentioned in the article ! i think the connection to pseudo ##sc ##ience needs to be clarified , or the article made more general and less creation ##ism - specific and the category tag removed entirely . [SEP],[CLS] ( and if such phrase exists , it would be provided by search engine even if mentioned page is not available as a whole ) [SEP]\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7ff5da21fa70>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n",
       "  (0): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (3): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (1): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (2): BertLayer(\n",
       "    (attention): BertAttention(\n",
       "      (self): BertSelfAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (output): BertSelfOutput(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (intermediate): BertIntermediate(\n",
       "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "    )\n",
       "    (output): BertOutput(\n",
       "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): Dropout(p=0.1, inplace=False)\n",
       "  (1): Linear(in_features=768, out_features=6, bias=True)\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.save('head-2')\n",
    "learner.load('head-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now unfreeze the entire model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.32E-06\n",
      "Min loss divided by 10: 6.31E-08\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.unfreeze()\n",
    "learner.lr_find()\n",
    "learner.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.031875</td>\n",
       "      <td>0.036496</td>\n",
       "      <td>0.981560</td>\n",
       "      <td>21:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.029987</td>\n",
       "      <td>0.037208</td>\n",
       "      <td>0.982291</td>\n",
       "      <td>18:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner.fit_one_cycle(2, slice(5e-6, 5e-5), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now see our model's prediction power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultiCategory ,\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([2.0871e-04, 2.5996e-05, 7.2458e-05, 1.2313e-05, 2.8451e-05, 3.4274e-05]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'you are so sweet'\n",
    "learner.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultiCategory toxic;obscene;insult,\n",
       " tensor([1., 0., 1., 0., 1., 0.]),\n",
       " tensor([9.9683e-01, 1.6824e-01, 9.8143e-01, 6.4289e-04, 9.7734e-01, 1.1376e-02]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'you are pathetic piece of shit'\n",
    "learner.predict(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is awesome!\n",
    "\n",
    "With few number of epochs, we are able to get the accuracy of around 98% on this multi-label classification task.\n",
    "\n",
    "Now, lets see how does Fastai ULMFiT fare on this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastai - ULMFiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have two parts:\n",
    "\n",
    "1. Training the Language Model\n",
    "2. Training the Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important thing to remember in the Language Model is that we train it without label. The basic objective by training language model is to predict the next sentence / words in a sequence of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lm = ItemLists(path, TextList.from_df(train, path=\".\", cols = \"comment_text\"), \n",
    "                   TextList.from_df(val, path=\".\", cols = 'comment_text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = src_lm.label_for_lm().databunch(bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>there would be no requirement to acknowledge whether you had a previous account ( xxmaj carlos xxmaj xxunk did not have a good record ) or not and i would then remove the sockpuppet template as irrelevant . xxup wp : xxup coi permits people to edit those articles , such as msjapan does , but just means you have to be more careful in ensuring that references back your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>'s an obsession . xxmaj what 's your point ? xxmaj what are your obsessions doing in article space ? : that is not a good reason to revert one 's edits . xxmaj additionally , the deaths of xxmaj xxunk 's companions are not listed alphabetically , but also not listed by who was born first , which is why i switched the positioning of the two . xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>faggot \\n  thanks for watching wiki raid threads and ruining fun , dick faggot \\n  thanks for watching wiki raid threads and ruining fun , dick faggot \\n  thanks for watching wiki raid threads and ruining fun , dick faggot \\n  thanks for watching wiki raid threads and ruining fun , dick faggot \\n  thanks for watching wiki raid threads and ruining fun , dick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>with your conclusions ( and also that , even though you do n't say anything clearly , i can see what might be your opinion about this xxup ip ) and i will no longer make sockpuppetry accusations in edit summaries . i just hope that xxmaj buspar will listen to your xxunk and will also stop harassing me through edit wars . xxbos xxmaj just wanted to let you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>i suspect to be wrong . xxmaj there exist many devices that are called solid - state an do in fact contain xxunk . \\n \\n  xxmaj another xxmaj question arises about the definition \" \" moving parts \" \" : xxmaj there are devices that contain keys and switches and xxunk , all of them sporting moving ( or movable ) parts and the devices are regarded solid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3, model_dir=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 1.10E-02\n",
      "Min loss divided by 10: 1.20E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.083547</td>\n",
       "      <td>3.923566</td>\n",
       "      <td>0.320891</td>\n",
       "      <td>08:55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, max_lr=slice(5e-4, 5e-3), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageLearner(data=TextLMDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj grandma xxmaj terri xxmaj should xxmaj burn in xxmaj trash \n",
       "  xxmaj grandma xxmaj terri is trash . i hate xxmaj grandma xxmaj terri . xxup xxunk her to xxup hell ! 71.74.76.40,xxbos , 9 xxmaj may 2009 ( xxup utc ) \n",
       "  xxmaj it would be easiest if you were to admit to being a member of the involved xxmaj portuguese xxmaj lodge , and then there would be no requirement to acknowledge whether you had a previous account ( xxmaj carlos xxmaj xxunk did not have a good record ) or not and i would then remove the sockpuppet template as irrelevant . xxup wp : xxup coi permits people to edit those articles , such as msjapan does , but just means you have to be more careful in ensuring that references back your edits and that xxup npov is upheld . 20:29,xxbos \" \n",
       " \n",
       "  xxmaj the xxmaj objectivity of this xxmaj discussion is doubtful ( non - existent ) \n",
       " \n",
       "  ( 1 ) xxmaj as indicated earlier , the section on xxmaj marxist leaders ’ views is misleading : \n",
       " \n",
       "  ( a ) it lays unwarranted and excessive emphasis on xxmaj trotsky , creating the misleading impression that other prominent xxmaj marxists ( xxmaj marx , xxmaj engels , xxmaj lenin ) did not advocate and / or practiced terrorism ; \n",
       " \n",
       "  ( b ) it lays unwarranted and excessive emphasis on the theoretical “ rejection of individual terrorism ” , creating the misleading impression that this is the main ( only ) xxmaj marxist position on terrorism . \n",
       " \n",
       "  ( 2 ) xxmaj the discussion is not being properly monitored : \n",
       " \n",
       "  ( a ) no discernible attempt is being made to establish and maintain an acceptable degree of objectivity ; \n",
       " \n",
       "  ( b ) important and relevant scholarly works such as the xxmaj international xxmaj encyclopedia of xxmaj terrorism are being ignored or xxunk excluded from the discussion ; \n",
       " \n",
       "  ( c ) though the only logical way to remedy the blatant imbalance in the above section is to include quotes by / on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with impunity by the apologists for xxmaj marxist terrorism who have done their best to sabotage and wreck both the article and the discussion . \n",
       " \n",
       "  ( 3 ) xxmaj among the tactics deployed by the apologist wreckers and xxunk the following may be identified as representative examples : \n",
       " \n",
       "  ( a ) it is claimed that xxmaj marx and xxmaj engels did not advocate terrorism despite the fact that scholarly works like the xxmaj international xxmaj encyclopedia of xxmaj terrorism show that they did , and xxmaj marx himself was known as “ xxmaj the xxmaj red xxmaj terror xxmaj doctor ” ; \n",
       " \n",
       "  ( b ) it is claimed that xxmaj marx and xxmaj engels were not involved in terrorist activities despite the fact that numerous sources from xxmaj the xxmaj neue xxmaj xxunk xxmaj zeitung to xxmaj isaiah xxmaj berlin and xxmaj francis xxmaj xxunk state otherwise ; \n",
       " \n",
       "  ( c ) it is claimed that xxmaj lenin does not refer to terror in xxmaj the xxmaj proletarian xxmaj revolution and the xxmaj renegade xxup k. xxmaj xxunk and other works / statements despite the fact that xxmaj robert xxmaj service , xxup iet , and other scholarly and reliable sources state that he does ; \n",
       " \n",
       "  ( d ) it is claimed that the xxmaj russian word ‘ ’ xxunk ’ ’ does not mean “ terror ” when : \n",
       " \n",
       "  i. the xxmaj oxford xxmaj russian xxmaj dictionary says that it does ; \n",
       " \n",
       "  ii . it is evident from the context that this is the case ; \n",
       " \n",
       "  iii . any educated xxmaj russian speaker can confirm that xxunk may mean “ terror ” depending on the context ; \n",
       " \n",
       "  ( e ) it is claimed that xxmaj marxism is “ scientific ” when in fact : \n",
       " \n",
       "  i. xxmaj marx was not a scientist ; \n",
       " \n",
       "  ii . xxmaj marx ’s background was philosophy and law , not science ; \n",
       " \n",
       "  iii . xxmaj marxism is not recognized as a science by the academic world ; \n",
       " \n",
       "  iv . virtually every one of xxmaj marx ’s predictions turned out to be wrong , as became increasingly apparent during his lifetime and xxunk so after his death ( xxup r. xxmaj pipes , xxmaj communism : a xxmaj brief xxmaj history , 2001 , p. 15 ) from which it follows that xxmaj marxism does not qualify as a scientific system by any accepted standards ; \n",
       " \n",
       "  v. the evidence indicates that xxmaj marxism is closer to a religious sect than to science proper ; \n",
       " \n",
       "  ( f ) apologist literature is being quoted in a fraudulent attempt to whitewash xxmaj marxist terrorism , in effect turning the discussion into an advertisement for terrorism ; \n",
       " \n",
       "  ( g ) it is claimed that xxmaj marxist terrorism is not rooted in the xxmaj marxist theory of class struggle even though there are numerous sources showing that it is ( please note that it is immaterial whether terrorism had already been justified in terms of a theory of class prior to xxmaj marx , the point being that it was advocated / practiced on the basis of xxmaj marxist class - struggle theories xxup by xxup marxists ) : \n",
       " \n",
       "  “ xxmaj karl xxmaj marx felt that terror was a necessary part of a revolutionary strategy ” ( xxmaj peter xxmaj xxunk , “ xxmaj theories of xxmaj terror in xxmaj urban xxmaj xxunk ” , xxup iet , p. 138 ) ; \n",
       " \n",
       "  “ xxmaj revolutionary terrorism has its roots in a political ideology , from the xxmaj marxist - xxmaj leninist thinking of the xxmaj left , to the fascists found on the xxmaj right ” ( xxmaj xxunk xxmaj gal - xxmaj or , \" \" xxmaj revolutionary xxmaj terrorism \" \" , xxup iet , p. 203 ) ; \n",
       " \n",
       "  “ … perhaps the most important key to xxmaj stalin ’s motivation lies in the realm of ideology . xxmaj the xxunk of xxmaj soviet communist ideology in the 1920s and 1930s was class struggle – the xxunk antagonism between mutually incompatible economic interest groups ” ( xxmaj geoffrey xxmaj robert , xxmaj stalins xxmaj wars , 2006 , pp . 17 - 18 ) ; \n",
       " \n",
       "  this fact is supported not only by reliable academic sources , but by elementary logic : \n",
       " \n",
       "  “ xxmaj in 1907 xxmaj xxunk published in the magazine ‘ ’ xxmaj neue xxmaj zeit ( xxmaj vol . xxup xxv 2 , p. 164 ) extracts from a letter by xxmaj marx to xxmaj xxunk dated xxmaj march 5 , 1852 . xxmaj in this letter , among other things , is the following noteworthy observation : … class struggle necessarily leads to the dictatorship of the proletariat … ”,xxbos xxmaj shelly xxmaj shock \n",
       "  xxmaj shelly xxmaj shock is . . . ( ),xxbos i do not care . xxmaj refer to xxmaj ong xxmaj teng xxmaj cheong talk page . xxmaj is xxmaj la goutte de pluie writing a biography or writing the history of trade unions . xxmaj she is making use of the dead to push her agenda again . xxmaj right before elections too . xxmaj how timely . xxunk\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: LMTextList\n",
       "xxbos xxmaj geez , are you xxunk ! xxmaj we 've already discussed why xxmaj marx was not an anarchist , i.e. he wanted to use a xxmaj state to mold his ' socialist man . ' xxmaj ergo , he is a statist - the opposite of an anarchist . i know a guy who says that , when he gets old and his teeth fall out , he 'll quit eating meat . xxmaj would you call him a vegetarian ?,xxbos xxmaj xxunk xxup rfa \n",
       " \n",
       "  xxmaj thanks for your support on my request for adminship . \n",
       " \n",
       "  xxmaj the final outcome was ( 31 / 4 / 1 ) , so i am now an administrator . xxmaj if you have any comments or concerns on my actions as an administrator , please let me know . xxmaj thank you !,xxbos \" \n",
       " \n",
       "  xxmaj birthday \n",
       " \n",
       "  xxmaj no worries , xxmaj it 's what i do ; ) xxmaj enjoy ur xxunk \",xxbos xxmaj pseudoscience category ? \n",
       " \n",
       "  i 'm assuming that this article is in the pseudoscience category because of its association with creationism . xxmaj however , there are modern , scientifically - accepted variants of xxunk that have nothing to do with creationism — and they 're even mentioned in the article ! i think the connection to pseudoscience needs to be clarified , or the article made more general and less creationism - specific and the category tag removed entirely .,xxbos ( and if such phrase exists , it would be provided by search engine even if mentioned page is not available as a whole )\n",
       "y: LMLabelList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(57520, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(57520, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=57520, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7ff5da21f9e0>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): Embedding(57520, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(57520, 400, padding_idx=1)\n",
       "  )\n",
       "  (2): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=57520, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('fit_head')\n",
    "learn.load('fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 5.75E-04\n",
      "Min loss divided by 10: 4.37E-04\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.701277</td>\n",
       "      <td>3.711765</td>\n",
       "      <td>0.346194</td>\n",
       "      <td>10:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.658217</td>\n",
       "      <td>3.638579</td>\n",
       "      <td>0.359605</td>\n",
       "      <td>09:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.628577</td>\n",
       "      <td>3.588710</td>\n",
       "      <td>0.368404</td>\n",
       "      <td>09:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.596001</td>\n",
       "      <td>3.560334</td>\n",
       "      <td>0.373193</td>\n",
       "      <td>10:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.461856</td>\n",
       "      <td>3.542283</td>\n",
       "      <td>0.375937</td>\n",
       "      <td>09:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.386634</td>\n",
       "      <td>3.532538</td>\n",
       "      <td>0.377337</td>\n",
       "      <td>09:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.320113</td>\n",
       "      <td>3.527869</td>\n",
       "      <td>0.378564</td>\n",
       "      <td>09:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.315945</td>\n",
       "      <td>3.526594</td>\n",
       "      <td>0.378836</td>\n",
       "      <td>09:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.212142</td>\n",
       "      <td>3.528076</td>\n",
       "      <td>0.378893</td>\n",
       "      <td>09:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.174291</td>\n",
       "      <td>3.528631</td>\n",
       "      <td>0.378844</td>\n",
       "      <td>10:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, max_lr = slice(1e-4, 1e-3), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4,  1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fine-tuned')\n",
    "learn.load('fine-tuned')\n",
    "learn.save_encoder('fine-tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"He is a piece of\"\n",
    "N_WORDS = 10\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a piece of shit . \n",
      " \n",
      "  The admins are not being directed\n",
      "He is a piece of shit . He 's a fucking hero . xxbos\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_clas = ItemLists(path, TextList.from_df( train, path=\".\", cols=\"comment_text\", vocab = data_lm.vocab),\n",
    "                    TextList.from_df( val, path=\".\", cols=\"comment_text\", vocab = data_lm.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas = src_clas.label_from_df(cols=label_cols).databunch(bs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj take that ! \\n \\n  xxup in xxup the xxup ass xxup in xxup the xxup ass xxup in xxup the xxup ass xxup in xxup the xxup ass xxup in xxup the xxup ass xxup in xxup the xxup ass xxup in xxup the xxup ass xxup in xxup the xxup ass xxup in xxup the xxup ass xxup in xxup the xxup ass xxup in</td>\n",
       "      <td>toxic;severe_toxic;obscene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos \" \\n \\n \\n  xxup mongo , tell us . xxup why xxup did xxup you xxup do xxup wtc ? xxmaj why xxup mongo ? xxup mongo , tell us . xxup why xxup did xxup you xxup do xxup wtc ? xxmaj why xxup mongo ? xxup mongo , tell us . xxup why xxup did xxup you xxup do xxup wtc ? xxmaj why xxup</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj j.delanoy xxup is a xxup nigger xxrep 4 = xxmaj j.delanoy xxup is a xxup nigger xxrep 4 = xxmaj j.delanoy xxup is a xxup nigger xxrep 4 = xxmaj j.delanoy xxup is a xxup nigger xxrep 4 = xxmaj j.delanoy xxup is a xxup nigger xxrep 4 = xxmaj j.delanoy xxup is a xxup nigger xxrep 4 = xxmaj j.delanoy xxup is a xxup nigger xxrep 4</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxup anyone xxup who xxup supports xxup this xxup is xxup fucking xxup sick . xxup men xxup and xxup underaged xxup fucking xxup kids xxrep 30 ? xxup what xxup in xxup the xxup fuck xxup you xxup all xxup should xxup be xxup shot xxup and xxup killed xxup you xxup bunch xxup of xxup fucking xxup faggot xxup pedophiles \\n \\n  xxwrep 4 xxup die</td>\n",
       "      <td>toxic;severe_toxic;obscene;threat;insult;identity_hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos \" \\n \\n  regarding a wiki writer \\n \\n  xxmaj regarding the writer at http : / / rexcurry.net / wikipedialies.html and referenced at http : / / en.wikipedia.org / wiki / xxmaj talk : xxmaj hitler_salute please note the following : xxmaj mr. xxmaj barlow is a nutter with an obsession . xxmaj the history of the salute is now improved in the xxmaj roman salute</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "xxbos xxmaj grandma xxmaj terri xxmaj should xxmaj burn in xxmaj trash \n",
       "  xxmaj grandma xxmaj terri is trash . i hate xxmaj grandma xxmaj terri . xxup xxunk her to xxup hell ! 71.74.76.40,xxbos , 9 xxmaj may 2009 ( xxup utc ) \n",
       "  xxmaj it would be easiest if you were to admit to being a member of the involved xxmaj portuguese xxmaj lodge , and then there would be no requirement to acknowledge whether you had a previous account ( xxmaj carlos xxmaj xxunk did not have a good record ) or not and i would then remove the sockpuppet template as irrelevant . xxup wp : xxup coi permits people to edit those articles , such as msjapan does , but just means you have to be more careful in ensuring that references back your edits and that xxup npov is upheld . 20:29,xxbos \" \n",
       " \n",
       "  xxmaj the xxmaj objectivity of this xxmaj discussion is doubtful ( non - existent ) \n",
       " \n",
       "  ( 1 ) xxmaj as indicated earlier , the section on xxmaj marxist leaders ’ views is misleading : \n",
       " \n",
       "  ( a ) it lays unwarranted and excessive emphasis on xxmaj trotsky , creating the misleading impression that other prominent xxmaj marxists ( xxmaj marx , xxmaj engels , xxmaj lenin ) did not advocate and / or practiced terrorism ; \n",
       " \n",
       "  ( b ) it lays unwarranted and excessive emphasis on the theoretical “ rejection of individual terrorism ” , creating the misleading impression that this is the main ( only ) xxmaj marxist position on terrorism . \n",
       " \n",
       "  ( 2 ) xxmaj the discussion is not being properly monitored : \n",
       " \n",
       "  ( a ) no discernible attempt is being made to establish and maintain an acceptable degree of objectivity ; \n",
       " \n",
       "  ( b ) important and relevant scholarly works such as the xxmaj international xxmaj encyclopedia of xxmaj terrorism are being ignored or xxunk excluded from the discussion ; \n",
       " \n",
       "  ( c ) though the only logical way to remedy the blatant imbalance in the above section is to include quotes by / on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with impunity by the apologists for xxmaj marxist terrorism who have done their best to sabotage and wreck both the article and the discussion . \n",
       " \n",
       "  ( 3 ) xxmaj among the tactics deployed by the apologist wreckers and xxunk the following may be identified as representative examples : \n",
       " \n",
       "  ( a ) it is claimed that xxmaj marx and xxmaj engels did not advocate terrorism despite the fact that scholarly works like the xxmaj international xxmaj encyclopedia of xxmaj terrorism show that they did , and xxmaj marx himself was known as “ xxmaj the xxmaj red xxmaj terror xxmaj doctor ” ; \n",
       " \n",
       "  ( b ) it is claimed that xxmaj marx and xxmaj engels were not involved in terrorist activities despite the fact that numerous sources from xxmaj the xxmaj neue xxmaj xxunk xxmaj zeitung to xxmaj isaiah xxmaj berlin and xxmaj francis xxmaj xxunk state otherwise ; \n",
       " \n",
       "  ( c ) it is claimed that xxmaj lenin does not refer to terror in xxmaj the xxmaj proletarian xxmaj revolution and the xxmaj renegade xxup k. xxmaj xxunk and other works / statements despite the fact that xxmaj robert xxmaj service , xxup iet , and other scholarly and reliable sources state that he does ; \n",
       " \n",
       "  ( d ) it is claimed that the xxmaj russian word ‘ ’ xxunk ’ ’ does not mean “ terror ” when : \n",
       " \n",
       "  i. the xxmaj oxford xxmaj russian xxmaj dictionary says that it does ; \n",
       " \n",
       "  ii . it is evident from the context that this is the case ; \n",
       " \n",
       "  iii . any educated xxmaj russian speaker can confirm that xxunk may mean “ terror ” depending on the context ; \n",
       " \n",
       "  ( e ) it is claimed that xxmaj marxism is “ scientific ” when in fact : \n",
       " \n",
       "  i. xxmaj marx was not a scientist ; \n",
       " \n",
       "  ii . xxmaj marx ’s background was philosophy and law , not science ; \n",
       " \n",
       "  iii . xxmaj marxism is not recognized as a science by the academic world ; \n",
       " \n",
       "  iv . virtually every one of xxmaj marx ’s predictions turned out to be wrong , as became increasingly apparent during his lifetime and xxunk so after his death ( xxup r. xxmaj pipes , xxmaj communism : a xxmaj brief xxmaj history , 2001 , p. 15 ) from which it follows that xxmaj marxism does not qualify as a scientific system by any accepted standards ; \n",
       " \n",
       "  v. the evidence indicates that xxmaj marxism is closer to a religious sect than to science proper ; \n",
       " \n",
       "  ( f ) apologist literature is being quoted in a fraudulent attempt to whitewash xxmaj marxist terrorism , in effect turning the discussion into an advertisement for terrorism ; \n",
       " \n",
       "  ( g ) it is claimed that xxmaj marxist terrorism is not rooted in the xxmaj marxist theory of class struggle even though there are numerous sources showing that it is ( please note that it is immaterial whether terrorism had already been justified in terms of a theory of class prior to xxmaj marx , the point being that it was advocated / practiced on the basis of xxmaj marxist class - struggle theories xxup by xxup marxists ) : \n",
       " \n",
       "  “ xxmaj karl xxmaj marx felt that terror was a necessary part of a revolutionary strategy ” ( xxmaj peter xxmaj xxunk , “ xxmaj theories of xxmaj terror in xxmaj urban xxmaj xxunk ” , xxup iet , p. 138 ) ; \n",
       " \n",
       "  “ xxmaj revolutionary terrorism has its roots in a political ideology , from the xxmaj marxist - xxmaj leninist thinking of the xxmaj left , to the fascists found on the xxmaj right ” ( xxmaj xxunk xxmaj gal - xxmaj or , \" \" xxmaj revolutionary xxmaj terrorism \" \" , xxup iet , p. 203 ) ; \n",
       " \n",
       "  “ … perhaps the most important key to xxmaj stalin ’s motivation lies in the realm of ideology . xxmaj the xxunk of xxmaj soviet communist ideology in the 1920s and 1930s was class struggle – the xxunk antagonism between mutually incompatible economic interest groups ” ( xxmaj geoffrey xxmaj robert , xxmaj stalins xxmaj wars , 2006 , pp . 17 - 18 ) ; \n",
       " \n",
       "  this fact is supported not only by reliable academic sources , but by elementary logic : \n",
       " \n",
       "  “ xxmaj in 1907 xxmaj xxunk published in the magazine ‘ ’ xxmaj neue xxmaj zeit ( xxmaj vol . xxup xxv 2 , p. 164 ) extracts from a letter by xxmaj marx to xxmaj xxunk dated xxmaj march 5 , 1852 . xxmaj in this letter , among other things , is the following noteworthy observation : … class struggle necessarily leads to the dictatorship of the proletariat … ”,xxbos xxmaj shelly xxmaj shock \n",
       "  xxmaj shelly xxmaj shock is . . . ( ),xxbos i do not care . xxmaj refer to xxmaj ong xxmaj teng xxmaj cheong talk page . xxmaj is xxmaj la goutte de pluie writing a biography or writing the history of trade unions . xxmaj she is making use of the dead to push her agenda again . xxmaj right before elections too . xxmaj how timely . xxunk\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "xxbos xxmaj geez , are you xxunk ! xxmaj we 've already discussed why xxmaj marx was not an anarchist , i.e. he wanted to use a xxmaj state to mold his ' socialist man . ' xxmaj ergo , he is a statist - the opposite of an anarchist . i know a guy who says that , when he gets old and his teeth fall out , he 'll quit eating meat . xxmaj would you call him a vegetarian ?,xxbos xxmaj xxunk xxup rfa \n",
       " \n",
       "  xxmaj thanks for your support on my request for adminship . \n",
       " \n",
       "  xxmaj the final outcome was ( 31 / 4 / 1 ) , so i am now an administrator . xxmaj if you have any comments or concerns on my actions as an administrator , please let me know . xxmaj thank you !,xxbos \" \n",
       " \n",
       "  xxmaj birthday \n",
       " \n",
       "  xxmaj no worries , xxmaj it 's what i do ; ) xxmaj enjoy ur xxunk \",xxbos xxmaj pseudoscience category ? \n",
       " \n",
       "  i 'm assuming that this article is in the pseudoscience category because of its association with creationism . xxmaj however , there are modern , scientifically - accepted variants of xxunk that have nothing to do with creationism — and they 're even mentioned in the article ! i think the connection to pseudoscience needs to be clarified , or the article made more general and less creationism - specific and the category tag removed entirely .,xxbos ( and if such phrase exists , it would be provided by search engine even if mentioned page is not available as a whole )\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(57520, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(57520, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7ff5da21fa70>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(57520, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(57520, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, model_dir='model', metrics=acc_02, loss_func=loss_func)\n",
    "learn.load_encoder('fine-tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 6.92E-02\n",
      "Min loss divided by 10: 6.31E-02\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.068887</td>\n",
       "      <td>0.063122</td>\n",
       "      <td>0.972207</td>\n",
       "      <td>04:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.061146</td>\n",
       "      <td>0.062645</td>\n",
       "      <td>0.970328</td>\n",
       "      <td>04:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, max_lr=slice(1e-3, 1e-2), moms=(0.8, 0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "xxbos xxmaj grandma xxmaj terri xxmaj should xxmaj burn in xxmaj trash \n",
       "  xxmaj grandma xxmaj terri is trash . i hate xxmaj grandma xxmaj terri . xxup xxunk her to xxup hell ! 71.74.76.40,xxbos , 9 xxmaj may 2009 ( xxup utc ) \n",
       "  xxmaj it would be easiest if you were to admit to being a member of the involved xxmaj portuguese xxmaj lodge , and then there would be no requirement to acknowledge whether you had a previous account ( xxmaj carlos xxmaj xxunk did not have a good record ) or not and i would then remove the sockpuppet template as irrelevant . xxup wp : xxup coi permits people to edit those articles , such as msjapan does , but just means you have to be more careful in ensuring that references back your edits and that xxup npov is upheld . 20:29,xxbos \" \n",
       " \n",
       "  xxmaj the xxmaj objectivity of this xxmaj discussion is doubtful ( non - existent ) \n",
       " \n",
       "  ( 1 ) xxmaj as indicated earlier , the section on xxmaj marxist leaders ’ views is misleading : \n",
       " \n",
       "  ( a ) it lays unwarranted and excessive emphasis on xxmaj trotsky , creating the misleading impression that other prominent xxmaj marxists ( xxmaj marx , xxmaj engels , xxmaj lenin ) did not advocate and / or practiced terrorism ; \n",
       " \n",
       "  ( b ) it lays unwarranted and excessive emphasis on the theoretical “ rejection of individual terrorism ” , creating the misleading impression that this is the main ( only ) xxmaj marxist position on terrorism . \n",
       " \n",
       "  ( 2 ) xxmaj the discussion is not being properly monitored : \n",
       " \n",
       "  ( a ) no discernible attempt is being made to establish and maintain an acceptable degree of objectivity ; \n",
       " \n",
       "  ( b ) important and relevant scholarly works such as the xxmaj international xxmaj encyclopedia of xxmaj terrorism are being ignored or xxunk excluded from the discussion ; \n",
       " \n",
       "  ( c ) though the only logical way to remedy the blatant imbalance in the above section is to include quotes by / on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with impunity by the apologists for xxmaj marxist terrorism who have done their best to sabotage and wreck both the article and the discussion . \n",
       " \n",
       "  ( 3 ) xxmaj among the tactics deployed by the apologist wreckers and xxunk the following may be identified as representative examples : \n",
       " \n",
       "  ( a ) it is claimed that xxmaj marx and xxmaj engels did not advocate terrorism despite the fact that scholarly works like the xxmaj international xxmaj encyclopedia of xxmaj terrorism show that they did , and xxmaj marx himself was known as “ xxmaj the xxmaj red xxmaj terror xxmaj doctor ” ; \n",
       " \n",
       "  ( b ) it is claimed that xxmaj marx and xxmaj engels were not involved in terrorist activities despite the fact that numerous sources from xxmaj the xxmaj neue xxmaj xxunk xxmaj zeitung to xxmaj isaiah xxmaj berlin and xxmaj francis xxmaj xxunk state otherwise ; \n",
       " \n",
       "  ( c ) it is claimed that xxmaj lenin does not refer to terror in xxmaj the xxmaj proletarian xxmaj revolution and the xxmaj renegade xxup k. xxmaj xxunk and other works / statements despite the fact that xxmaj robert xxmaj service , xxup iet , and other scholarly and reliable sources state that he does ; \n",
       " \n",
       "  ( d ) it is claimed that the xxmaj russian word ‘ ’ xxunk ’ ’ does not mean “ terror ” when : \n",
       " \n",
       "  i. the xxmaj oxford xxmaj russian xxmaj dictionary says that it does ; \n",
       " \n",
       "  ii . it is evident from the context that this is the case ; \n",
       " \n",
       "  iii . any educated xxmaj russian speaker can confirm that xxunk may mean “ terror ” depending on the context ; \n",
       " \n",
       "  ( e ) it is claimed that xxmaj marxism is “ scientific ” when in fact : \n",
       " \n",
       "  i. xxmaj marx was not a scientist ; \n",
       " \n",
       "  ii . xxmaj marx ’s background was philosophy and law , not science ; \n",
       " \n",
       "  iii . xxmaj marxism is not recognized as a science by the academic world ; \n",
       " \n",
       "  iv . virtually every one of xxmaj marx ’s predictions turned out to be wrong , as became increasingly apparent during his lifetime and xxunk so after his death ( xxup r. xxmaj pipes , xxmaj communism : a xxmaj brief xxmaj history , 2001 , p. 15 ) from which it follows that xxmaj marxism does not qualify as a scientific system by any accepted standards ; \n",
       " \n",
       "  v. the evidence indicates that xxmaj marxism is closer to a religious sect than to science proper ; \n",
       " \n",
       "  ( f ) apologist literature is being quoted in a fraudulent attempt to whitewash xxmaj marxist terrorism , in effect turning the discussion into an advertisement for terrorism ; \n",
       " \n",
       "  ( g ) it is claimed that xxmaj marxist terrorism is not rooted in the xxmaj marxist theory of class struggle even though there are numerous sources showing that it is ( please note that it is immaterial whether terrorism had already been justified in terms of a theory of class prior to xxmaj marx , the point being that it was advocated / practiced on the basis of xxmaj marxist class - struggle theories xxup by xxup marxists ) : \n",
       " \n",
       "  “ xxmaj karl xxmaj marx felt that terror was a necessary part of a revolutionary strategy ” ( xxmaj peter xxmaj xxunk , “ xxmaj theories of xxmaj terror in xxmaj urban xxmaj xxunk ” , xxup iet , p. 138 ) ; \n",
       " \n",
       "  “ xxmaj revolutionary terrorism has its roots in a political ideology , from the xxmaj marxist - xxmaj leninist thinking of the xxmaj left , to the fascists found on the xxmaj right ” ( xxmaj xxunk xxmaj gal - xxmaj or , \" \" xxmaj revolutionary xxmaj terrorism \" \" , xxup iet , p. 203 ) ; \n",
       " \n",
       "  “ … perhaps the most important key to xxmaj stalin ’s motivation lies in the realm of ideology . xxmaj the xxunk of xxmaj soviet communist ideology in the 1920s and 1930s was class struggle – the xxunk antagonism between mutually incompatible economic interest groups ” ( xxmaj geoffrey xxmaj robert , xxmaj stalins xxmaj wars , 2006 , pp . 17 - 18 ) ; \n",
       " \n",
       "  this fact is supported not only by reliable academic sources , but by elementary logic : \n",
       " \n",
       "  “ xxmaj in 1907 xxmaj xxunk published in the magazine ‘ ’ xxmaj neue xxmaj zeit ( xxmaj vol . xxup xxv 2 , p. 164 ) extracts from a letter by xxmaj marx to xxmaj xxunk dated xxmaj march 5 , 1852 . xxmaj in this letter , among other things , is the following noteworthy observation : … class struggle necessarily leads to the dictatorship of the proletariat … ”,xxbos xxmaj shelly xxmaj shock \n",
       "  xxmaj shelly xxmaj shock is . . . ( ),xxbos i do not care . xxmaj refer to xxmaj ong xxmaj teng xxmaj cheong talk page . xxmaj is xxmaj la goutte de pluie writing a biography or writing the history of trade unions . xxmaj she is making use of the dead to push her agenda again . xxmaj right before elections too . xxmaj how timely . xxunk\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "xxbos xxmaj geez , are you xxunk ! xxmaj we 've already discussed why xxmaj marx was not an anarchist , i.e. he wanted to use a xxmaj state to mold his ' socialist man . ' xxmaj ergo , he is a statist - the opposite of an anarchist . i know a guy who says that , when he gets old and his teeth fall out , he 'll quit eating meat . xxmaj would you call him a vegetarian ?,xxbos xxmaj xxunk xxup rfa \n",
       " \n",
       "  xxmaj thanks for your support on my request for adminship . \n",
       " \n",
       "  xxmaj the final outcome was ( 31 / 4 / 1 ) , so i am now an administrator . xxmaj if you have any comments or concerns on my actions as an administrator , please let me know . xxmaj thank you !,xxbos \" \n",
       " \n",
       "  xxmaj birthday \n",
       " \n",
       "  xxmaj no worries , xxmaj it 's what i do ; ) xxmaj enjoy ur xxunk \",xxbos xxmaj pseudoscience category ? \n",
       " \n",
       "  i 'm assuming that this article is in the pseudoscience category because of its association with creationism . xxmaj however , there are modern , scientifically - accepted variants of xxunk that have nothing to do with creationism — and they 're even mentioned in the article ! i think the connection to pseudoscience needs to be clarified , or the article made more general and less creationism - specific and the category tag removed entirely .,xxbos ( and if such phrase exists , it would be provided by search engine even if mentioned page is not available as a whole )\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(57520, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(57520, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7ff5da21fa70>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(57520, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(57520, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('first-head')\n",
    "learn.load('first-head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.064952</td>\n",
       "      <td>1.597279</td>\n",
       "      <td>0.973858</td>\n",
       "      <td>05:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.064196</td>\n",
       "      <td>1.406804</td>\n",
       "      <td>0.974944</td>\n",
       "      <td>05:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(2, slice(5e-2/(2.6**4),5e-2), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "xxbos xxmaj grandma xxmaj terri xxmaj should xxmaj burn in xxmaj trash \n",
       "  xxmaj grandma xxmaj terri is trash . i hate xxmaj grandma xxmaj terri . xxup xxunk her to xxup hell ! 71.74.76.40,xxbos , 9 xxmaj may 2009 ( xxup utc ) \n",
       "  xxmaj it would be easiest if you were to admit to being a member of the involved xxmaj portuguese xxmaj lodge , and then there would be no requirement to acknowledge whether you had a previous account ( xxmaj carlos xxmaj xxunk did not have a good record ) or not and i would then remove the sockpuppet template as irrelevant . xxup wp : xxup coi permits people to edit those articles , such as msjapan does , but just means you have to be more careful in ensuring that references back your edits and that xxup npov is upheld . 20:29,xxbos \" \n",
       " \n",
       "  xxmaj the xxmaj objectivity of this xxmaj discussion is doubtful ( non - existent ) \n",
       " \n",
       "  ( 1 ) xxmaj as indicated earlier , the section on xxmaj marxist leaders ’ views is misleading : \n",
       " \n",
       "  ( a ) it lays unwarranted and excessive emphasis on xxmaj trotsky , creating the misleading impression that other prominent xxmaj marxists ( xxmaj marx , xxmaj engels , xxmaj lenin ) did not advocate and / or practiced terrorism ; \n",
       " \n",
       "  ( b ) it lays unwarranted and excessive emphasis on the theoretical “ rejection of individual terrorism ” , creating the misleading impression that this is the main ( only ) xxmaj marxist position on terrorism . \n",
       " \n",
       "  ( 2 ) xxmaj the discussion is not being properly monitored : \n",
       " \n",
       "  ( a ) no discernible attempt is being made to establish and maintain an acceptable degree of objectivity ; \n",
       " \n",
       "  ( b ) important and relevant scholarly works such as the xxmaj international xxmaj encyclopedia of xxmaj terrorism are being ignored or xxunk excluded from the discussion ; \n",
       " \n",
       "  ( c ) though the only logical way to remedy the blatant imbalance in the above section is to include quotes by / on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with impunity by the apologists for xxmaj marxist terrorism who have done their best to sabotage and wreck both the article and the discussion . \n",
       " \n",
       "  ( 3 ) xxmaj among the tactics deployed by the apologist wreckers and xxunk the following may be identified as representative examples : \n",
       " \n",
       "  ( a ) it is claimed that xxmaj marx and xxmaj engels did not advocate terrorism despite the fact that scholarly works like the xxmaj international xxmaj encyclopedia of xxmaj terrorism show that they did , and xxmaj marx himself was known as “ xxmaj the xxmaj red xxmaj terror xxmaj doctor ” ; \n",
       " \n",
       "  ( b ) it is claimed that xxmaj marx and xxmaj engels were not involved in terrorist activities despite the fact that numerous sources from xxmaj the xxmaj neue xxmaj xxunk xxmaj zeitung to xxmaj isaiah xxmaj berlin and xxmaj francis xxmaj xxunk state otherwise ; \n",
       " \n",
       "  ( c ) it is claimed that xxmaj lenin does not refer to terror in xxmaj the xxmaj proletarian xxmaj revolution and the xxmaj renegade xxup k. xxmaj xxunk and other works / statements despite the fact that xxmaj robert xxmaj service , xxup iet , and other scholarly and reliable sources state that he does ; \n",
       " \n",
       "  ( d ) it is claimed that the xxmaj russian word ‘ ’ xxunk ’ ’ does not mean “ terror ” when : \n",
       " \n",
       "  i. the xxmaj oxford xxmaj russian xxmaj dictionary says that it does ; \n",
       " \n",
       "  ii . it is evident from the context that this is the case ; \n",
       " \n",
       "  iii . any educated xxmaj russian speaker can confirm that xxunk may mean “ terror ” depending on the context ; \n",
       " \n",
       "  ( e ) it is claimed that xxmaj marxism is “ scientific ” when in fact : \n",
       " \n",
       "  i. xxmaj marx was not a scientist ; \n",
       " \n",
       "  ii . xxmaj marx ’s background was philosophy and law , not science ; \n",
       " \n",
       "  iii . xxmaj marxism is not recognized as a science by the academic world ; \n",
       " \n",
       "  iv . virtually every one of xxmaj marx ’s predictions turned out to be wrong , as became increasingly apparent during his lifetime and xxunk so after his death ( xxup r. xxmaj pipes , xxmaj communism : a xxmaj brief xxmaj history , 2001 , p. 15 ) from which it follows that xxmaj marxism does not qualify as a scientific system by any accepted standards ; \n",
       " \n",
       "  v. the evidence indicates that xxmaj marxism is closer to a religious sect than to science proper ; \n",
       " \n",
       "  ( f ) apologist literature is being quoted in a fraudulent attempt to whitewash xxmaj marxist terrorism , in effect turning the discussion into an advertisement for terrorism ; \n",
       " \n",
       "  ( g ) it is claimed that xxmaj marxist terrorism is not rooted in the xxmaj marxist theory of class struggle even though there are numerous sources showing that it is ( please note that it is immaterial whether terrorism had already been justified in terms of a theory of class prior to xxmaj marx , the point being that it was advocated / practiced on the basis of xxmaj marxist class - struggle theories xxup by xxup marxists ) : \n",
       " \n",
       "  “ xxmaj karl xxmaj marx felt that terror was a necessary part of a revolutionary strategy ” ( xxmaj peter xxmaj xxunk , “ xxmaj theories of xxmaj terror in xxmaj urban xxmaj xxunk ” , xxup iet , p. 138 ) ; \n",
       " \n",
       "  “ xxmaj revolutionary terrorism has its roots in a political ideology , from the xxmaj marxist - xxmaj leninist thinking of the xxmaj left , to the fascists found on the xxmaj right ” ( xxmaj xxunk xxmaj gal - xxmaj or , \" \" xxmaj revolutionary xxmaj terrorism \" \" , xxup iet , p. 203 ) ; \n",
       " \n",
       "  “ … perhaps the most important key to xxmaj stalin ’s motivation lies in the realm of ideology . xxmaj the xxunk of xxmaj soviet communist ideology in the 1920s and 1930s was class struggle – the xxunk antagonism between mutually incompatible economic interest groups ” ( xxmaj geoffrey xxmaj robert , xxmaj stalins xxmaj wars , 2006 , pp . 17 - 18 ) ; \n",
       " \n",
       "  this fact is supported not only by reliable academic sources , but by elementary logic : \n",
       " \n",
       "  “ xxmaj in 1907 xxmaj xxunk published in the magazine ‘ ’ xxmaj neue xxmaj zeit ( xxmaj vol . xxup xxv 2 , p. 164 ) extracts from a letter by xxmaj marx to xxmaj xxunk dated xxmaj march 5 , 1852 . xxmaj in this letter , among other things , is the following noteworthy observation : … class struggle necessarily leads to the dictatorship of the proletariat … ”,xxbos xxmaj shelly xxmaj shock \n",
       "  xxmaj shelly xxmaj shock is . . . ( ),xxbos i do not care . xxmaj refer to xxmaj ong xxmaj teng xxmaj cheong talk page . xxmaj is xxmaj la goutte de pluie writing a biography or writing the history of trade unions . xxmaj she is making use of the dead to push her agenda again . xxmaj right before elections too . xxmaj how timely . xxunk\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "xxbos xxmaj geez , are you xxunk ! xxmaj we 've already discussed why xxmaj marx was not an anarchist , i.e. he wanted to use a xxmaj state to mold his ' socialist man . ' xxmaj ergo , he is a statist - the opposite of an anarchist . i know a guy who says that , when he gets old and his teeth fall out , he 'll quit eating meat . xxmaj would you call him a vegetarian ?,xxbos xxmaj xxunk xxup rfa \n",
       " \n",
       "  xxmaj thanks for your support on my request for adminship . \n",
       " \n",
       "  xxmaj the final outcome was ( 31 / 4 / 1 ) , so i am now an administrator . xxmaj if you have any comments or concerns on my actions as an administrator , please let me know . xxmaj thank you !,xxbos \" \n",
       " \n",
       "  xxmaj birthday \n",
       " \n",
       "  xxmaj no worries , xxmaj it 's what i do ; ) xxmaj enjoy ur xxunk \",xxbos xxmaj pseudoscience category ? \n",
       " \n",
       "  i 'm assuming that this article is in the pseudoscience category because of its association with creationism . xxmaj however , there are modern , scientifically - accepted variants of xxunk that have nothing to do with creationism — and they 're even mentioned in the article ! i think the connection to pseudoscience needs to be clarified , or the article made more general and less creationism - specific and the category tag removed entirely .,xxbos ( and if such phrase exists , it would be provided by search engine even if mentioned page is not available as a whole )\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(57520, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(57520, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7ff5da21fa70>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(57520, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(57520, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('second')\n",
    "learn.load('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.081041</td>\n",
       "      <td>6.929391</td>\n",
       "      <td>0.909776</td>\n",
       "      <td>06:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.059330</td>\n",
       "      <td>0.523655</td>\n",
       "      <td>0.977764</td>\n",
       "      <td>07:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(2, slice(5e-2/(2.6**4),5e-2), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (127656 items)\n",
       "x: TextList\n",
       "xxbos xxmaj grandma xxmaj terri xxmaj should xxmaj burn in xxmaj trash \n",
       "  xxmaj grandma xxmaj terri is trash . i hate xxmaj grandma xxmaj terri . xxup xxunk her to xxup hell ! 71.74.76.40,xxbos , 9 xxmaj may 2009 ( xxup utc ) \n",
       "  xxmaj it would be easiest if you were to admit to being a member of the involved xxmaj portuguese xxmaj lodge , and then there would be no requirement to acknowledge whether you had a previous account ( xxmaj carlos xxmaj xxunk did not have a good record ) or not and i would then remove the sockpuppet template as irrelevant . xxup wp : xxup coi permits people to edit those articles , such as msjapan does , but just means you have to be more careful in ensuring that references back your edits and that xxup npov is upheld . 20:29,xxbos \" \n",
       " \n",
       "  xxmaj the xxmaj objectivity of this xxmaj discussion is doubtful ( non - existent ) \n",
       " \n",
       "  ( 1 ) xxmaj as indicated earlier , the section on xxmaj marxist leaders ’ views is misleading : \n",
       " \n",
       "  ( a ) it lays unwarranted and excessive emphasis on xxmaj trotsky , creating the misleading impression that other prominent xxmaj marxists ( xxmaj marx , xxmaj engels , xxmaj lenin ) did not advocate and / or practiced terrorism ; \n",
       " \n",
       "  ( b ) it lays unwarranted and excessive emphasis on the theoretical “ rejection of individual terrorism ” , creating the misleading impression that this is the main ( only ) xxmaj marxist position on terrorism . \n",
       " \n",
       "  ( 2 ) xxmaj the discussion is not being properly monitored : \n",
       " \n",
       "  ( a ) no discernible attempt is being made to establish and maintain an acceptable degree of objectivity ; \n",
       " \n",
       "  ( b ) important and relevant scholarly works such as the xxmaj international xxmaj encyclopedia of xxmaj terrorism are being ignored or xxunk excluded from the discussion ; \n",
       " \n",
       "  ( c ) though the only logical way to remedy the blatant imbalance in the above section is to include quotes by / on other leaders who are known to have endorsed and practiced terrorism all attempts to do so have been systematically blocked with impunity by the apologists for xxmaj marxist terrorism who have done their best to sabotage and wreck both the article and the discussion . \n",
       " \n",
       "  ( 3 ) xxmaj among the tactics deployed by the apologist wreckers and xxunk the following may be identified as representative examples : \n",
       " \n",
       "  ( a ) it is claimed that xxmaj marx and xxmaj engels did not advocate terrorism despite the fact that scholarly works like the xxmaj international xxmaj encyclopedia of xxmaj terrorism show that they did , and xxmaj marx himself was known as “ xxmaj the xxmaj red xxmaj terror xxmaj doctor ” ; \n",
       " \n",
       "  ( b ) it is claimed that xxmaj marx and xxmaj engels were not involved in terrorist activities despite the fact that numerous sources from xxmaj the xxmaj neue xxmaj xxunk xxmaj zeitung to xxmaj isaiah xxmaj berlin and xxmaj francis xxmaj xxunk state otherwise ; \n",
       " \n",
       "  ( c ) it is claimed that xxmaj lenin does not refer to terror in xxmaj the xxmaj proletarian xxmaj revolution and the xxmaj renegade xxup k. xxmaj xxunk and other works / statements despite the fact that xxmaj robert xxmaj service , xxup iet , and other scholarly and reliable sources state that he does ; \n",
       " \n",
       "  ( d ) it is claimed that the xxmaj russian word ‘ ’ xxunk ’ ’ does not mean “ terror ” when : \n",
       " \n",
       "  i. the xxmaj oxford xxmaj russian xxmaj dictionary says that it does ; \n",
       " \n",
       "  ii . it is evident from the context that this is the case ; \n",
       " \n",
       "  iii . any educated xxmaj russian speaker can confirm that xxunk may mean “ terror ” depending on the context ; \n",
       " \n",
       "  ( e ) it is claimed that xxmaj marxism is “ scientific ” when in fact : \n",
       " \n",
       "  i. xxmaj marx was not a scientist ; \n",
       " \n",
       "  ii . xxmaj marx ’s background was philosophy and law , not science ; \n",
       " \n",
       "  iii . xxmaj marxism is not recognized as a science by the academic world ; \n",
       " \n",
       "  iv . virtually every one of xxmaj marx ’s predictions turned out to be wrong , as became increasingly apparent during his lifetime and xxunk so after his death ( xxup r. xxmaj pipes , xxmaj communism : a xxmaj brief xxmaj history , 2001 , p. 15 ) from which it follows that xxmaj marxism does not qualify as a scientific system by any accepted standards ; \n",
       " \n",
       "  v. the evidence indicates that xxmaj marxism is closer to a religious sect than to science proper ; \n",
       " \n",
       "  ( f ) apologist literature is being quoted in a fraudulent attempt to whitewash xxmaj marxist terrorism , in effect turning the discussion into an advertisement for terrorism ; \n",
       " \n",
       "  ( g ) it is claimed that xxmaj marxist terrorism is not rooted in the xxmaj marxist theory of class struggle even though there are numerous sources showing that it is ( please note that it is immaterial whether terrorism had already been justified in terms of a theory of class prior to xxmaj marx , the point being that it was advocated / practiced on the basis of xxmaj marxist class - struggle theories xxup by xxup marxists ) : \n",
       " \n",
       "  “ xxmaj karl xxmaj marx felt that terror was a necessary part of a revolutionary strategy ” ( xxmaj peter xxmaj xxunk , “ xxmaj theories of xxmaj terror in xxmaj urban xxmaj xxunk ” , xxup iet , p. 138 ) ; \n",
       " \n",
       "  “ xxmaj revolutionary terrorism has its roots in a political ideology , from the xxmaj marxist - xxmaj leninist thinking of the xxmaj left , to the fascists found on the xxmaj right ” ( xxmaj xxunk xxmaj gal - xxmaj or , \" \" xxmaj revolutionary xxmaj terrorism \" \" , xxup iet , p. 203 ) ; \n",
       " \n",
       "  “ … perhaps the most important key to xxmaj stalin ’s motivation lies in the realm of ideology . xxmaj the xxunk of xxmaj soviet communist ideology in the 1920s and 1930s was class struggle – the xxunk antagonism between mutually incompatible economic interest groups ” ( xxmaj geoffrey xxmaj robert , xxmaj stalins xxmaj wars , 2006 , pp . 17 - 18 ) ; \n",
       " \n",
       "  this fact is supported not only by reliable academic sources , but by elementary logic : \n",
       " \n",
       "  “ xxmaj in 1907 xxmaj xxunk published in the magazine ‘ ’ xxmaj neue xxmaj zeit ( xxmaj vol . xxup xxv 2 , p. 164 ) extracts from a letter by xxmaj marx to xxmaj xxunk dated xxmaj march 5 , 1852 . xxmaj in this letter , among other things , is the following noteworthy observation : … class struggle necessarily leads to the dictatorship of the proletariat … ”,xxbos xxmaj shelly xxmaj shock \n",
       "  xxmaj shelly xxmaj shock is . . . ( ),xxbos i do not care . xxmaj refer to xxmaj ong xxmaj teng xxmaj cheong talk page . xxmaj is xxmaj la goutte de pluie writing a biography or writing the history of trade unions . xxmaj she is making use of the dead to push her agenda again . xxmaj right before elections too . xxmaj how timely . xxunk\n",
       "y: MultiCategoryList\n",
       "toxic,,,,\n",
       "Path: .;\n",
       "\n",
       "Valid: LabelList (31915 items)\n",
       "x: TextList\n",
       "xxbos xxmaj geez , are you xxunk ! xxmaj we 've already discussed why xxmaj marx was not an anarchist , i.e. he wanted to use a xxmaj state to mold his ' socialist man . ' xxmaj ergo , he is a statist - the opposite of an anarchist . i know a guy who says that , when he gets old and his teeth fall out , he 'll quit eating meat . xxmaj would you call him a vegetarian ?,xxbos xxmaj xxunk xxup rfa \n",
       " \n",
       "  xxmaj thanks for your support on my request for adminship . \n",
       " \n",
       "  xxmaj the final outcome was ( 31 / 4 / 1 ) , so i am now an administrator . xxmaj if you have any comments or concerns on my actions as an administrator , please let me know . xxmaj thank you !,xxbos \" \n",
       " \n",
       "  xxmaj birthday \n",
       " \n",
       "  xxmaj no worries , xxmaj it 's what i do ; ) xxmaj enjoy ur xxunk \",xxbos xxmaj pseudoscience category ? \n",
       " \n",
       "  i 'm assuming that this article is in the pseudoscience category because of its association with creationism . xxmaj however , there are modern , scientifically - accepted variants of xxunk that have nothing to do with creationism — and they 're even mentioned in the article ! i think the connection to pseudoscience needs to be clarified , or the article made more general and less creationism - specific and the category tag removed entirely .,xxbos ( and if such phrase exists , it would be provided by search engine even if mentioned page is not available as a whole )\n",
       "y: MultiCategoryList\n",
       ",,,,\n",
       "Path: .;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(57520, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(57520, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=BCEWithLogitsLoss(), metrics=[functools.partial(<function accuracy_thresh at 0x7ff5da21fa70>, thresh=0.25)], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='model', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(57520, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(57520, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.save('third')\n",
    "learn.load('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "Min numerical gradient: 6.31E-07\n",
      "Min loss divided by 10: 1.32E-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy_thresh</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.057197</td>\n",
       "      <td>0.185493</td>\n",
       "      <td>0.976296</td>\n",
       "      <td>08:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.065297</td>\n",
       "      <td>0.405699</td>\n",
       "      <td>0.976943</td>\n",
       "      <td>08:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(2, slice(1e-4/(2.6**4),1e-4), moms=(0.8,0.7), pct_start=0.2, wd =(1e-7, 1e-5, 1e-4, 1e-3, 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultiCategory ,\n",
       " tensor([0., 0., 0., 0., 0., 0.]),\n",
       " tensor([0.0649, 0.0009, 0.0085, 0.0014, 0.0122, 0.0033]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict('she is so sweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MultiCategory toxic;obscene;insult,\n",
       " tensor([1., 0., 1., 0., 1., 0.]),\n",
       " tensor([0.9820, 0.3269, 0.9420, 0.0400, 0.8684, 0.1540]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.predict('you are pathetic piece of shit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
